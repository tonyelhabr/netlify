---
title: A Tidy Text Analysis of My Google Search History
date: "2018-02-16"
slug: tidy-text-analysis-google-search-history
categories: []
tags:
  - r
  - google
  - tidytext
  - ggplot2
  - dplyr
banner: "img/tidy-text-analysis-google-search-history/viz_corrs_network-tony-banner.png"
---

```{r setup, include = FALSE}
# knitr::opts_knit$set(root.dir = "content/post/tidy-text-analysis-google-search-history"))
knitr::opts_chunk$set(
  echo = TRUE,
  cache = FALSE,
  results = "markdown",
  fig.align = "center",
  fig.show = "asis",
  fig.width = 6,
  fig.height = 6,
  # out.width = 6,
  # out.height = 6,
  warning = FALSE,
  message = FALSE
)
```

While brainstorming about cool ways to practice text mining with R
I came up with the idea of exploring my own Google search history.
Then, after googling (ironically) if anyone had done something like this, I stumbled upon 
[Lisa Charlotte's blog post](https://lisacharlotterost.github.io/2015/06/20/Searching-through-the-years/). 
Lisa's post (actually, a series of posts) are from a while back, so
her instructions for how to download your personal Google history and the format
of the downloads (nowadays, it's in a .html file instead of a series of .json files)
are no longer applicable.

I googled a bit more and found a recent
[RPubs write-up by Stephanie Lancz](https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#) 
that not only included concise instructions on how/where to get personal Google data,
but also how to clean it with `R`!
With the hard work of figuring out how to set up the data provided for me,
I was excited to find out what I could do.

In this write-up (which
can be downloaded from GitHub and re-used for one's own analysis), I explore
different techniques for visualizing and understanding my data.
I do my best to implement methods that are generic and could be applied to any
kind of similar analysis, irregardless of the topic.
Much of my code is guided by the work of others, especially that of
David Robinson and Julia Silge, who have written an amazingly helpful book
on text analysis--
[_Tidy Text Mining with R_ book](https://www.tidytextmining.com/),
I provide references for my inspiration where appropriate.

## Setup


First, following "best practices", I import all of the packages that I'll be using.

```{r import_packages}
library("dplyr")
library("stringr")
library("xml2")
library("rvest")
library("lubridate")
library("viridis")
library("ggplot2")
library("tidytext")
library("tidyr")
library("ggalt")
library("widyr")
library("drlib")
library("igraph")
library("ggraph")
# library("topicmodels")
# devtools::install_github("tonyelhabr/temisc")
library("temisc") # Personal package.
```

Next, I create a `params` list in order to emulate what one might 
do with a parameterized RMarkdown report (where the `params` would be a 
part of the yaml header.

```{r define_params}
params <-
  list(
    filepath = "data-raw/Tony-My Activity-Search-MyActivity.html",
    name_main = "Tony",
    color_main = "firebrick"
  )
```

I'll also go ahead and create a couple of functions for coloring some of the
plots that I'll create. These can be customized to one's personal preferences.

```{r define_scale_x_funcs}
scale_color_func <- function() {
  viridis::scale_color_viridis(
    option = "D",
    discrete = TRUE,
    begin = 0,
    end = 0.75
  ) 
}

scale_fill_func <- function() {
  viridis::scale_fill_viridis(
    option = "D",
    discrete = TRUE,
    begin = 0,
    end = 0.75
  ) 
}
```



### Import and Clean

Then, on to the "dirty" work of importing and cleaning the data.
I don't deviate much from 
[Stephanie Lancz's methods](https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#)
for extracting data elements
from the .html file

```{r import_and_clean_data, eval = FALSE}
# Reference:
# + https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#

doc_html <- params$filepath
search_archive <- xml2::read_html(doc_html)

# Extract search time.
date_search <-
  search_archive %>%
  html_nodes(xpath = '//div[@class="mdl-grid"]/div/div') %>%
  str_extract(pattern = "(?<=<br>)(.*)(?<=PM|AM)") %>%
  mdy_hms()

# Extract search text.
text_search <-
  search_archive %>%
  html_nodes(xpath = '//div[@class="mdl-grid"]/div/div') %>%
  str_extract(pattern = '(?<=<a)(.*)(?=</a>)') %>%
  str_extract(pattern = '(?<=\">)(.*)')

# Extract search type.
type_search <-
  search_archive %>% 
  html_nodes(xpath = '//div[@class="mdl-grid"]/div/div') %>% 
  str_extract(pattern = "(?<=mdl-typography--body-1\">)(.*)(?=<a)") %>% 
  str_extract(pattern = "(\\w+)(?=\\s)")

# Differences from reference:
# + Using `lubridate::wday()` instead of calling `weekdays()` and coercing to factors.
# + Using `yyyy`, `mm`, `wd`, and `hh` instead of `year`, `month`, `wday`, and `hour`.
# + Convert `yyyy` to an integer (from a double).
# + Adding a `time` column to use for a later visualization.
# + Adding a `name` column to make this code more "parametric".
data <-
  tibble(
    name = params$name_main,
    timestamp = date_search,
    date = lubridate::as_date(date_search),
    yyyy = lubridate::year(date_search) %>% as.integer(),
    mm = lubridate::month(date_search, label = TRUE),
    wd = lubridate::wday(date_search, label = TRUE),
    hh = lubridate::hour(date_search),
    time = lubridate::hour(timestamp) + (lubridate::minute(timestamp) / 60),
    type = type_search,
    text = text_search
  )

```

Notably, there are some rows that did not get parsed correctly. I decide to exclude
them from the rest of the analysis.
Also, my first searches come at the end of 2010, and my most recent ones
(i.e. the ones just before I downloaded my data) come in the first month or so of 2018.
To make the aspect of my analysis that deal with years a bit "cleaner", I'll truncate
these ends so that my data spans the years 2011 through 2017.

```{r filter_data, eval = FALSE}
data %>% count(yyyy, sort = TRUE)
data <- data %>% filter(!is.na(yyyy))
data <- data %>% filter(!(yyyy %in% c(2010, 2018)))
```


```{r data_io, echo = FALSE}
filepath_data <- "https://raw.githubusercontent.com/tonyelhabr/googleactivity/master/data/data-tony-cleaned.csv"
# filepath_data <- "data-ext/data-tony-cleaned.rds"
# saveRDS(data, file = filepath_data)
# data <- readRDS(filepath_data)
# readr::write_csv(data, path = gsub("\\.rds", ".csv", filepath_data)
data <- readr::read_csv(filepath_data)
```

## Analysis

### Search Count Distributions

Next, it's time to start doing some basic exploratory data analysis (EDA).
Given the temporal nature of the data, an easy EDA approach to implement
is visualization across different time periods. To save some effort
(or, as I like to see it, make my code more efficient), we can create a 
helper function. (Notably, the `geom` to use is a parameter to this function.
Through experimentation, I found that `ggplot2::geom_bar()` seems
to work best with most temporal periods, with the exception of plotting `Date` variables,
where `ggplot2::geom_hist()` seems more appropriate.)

```{r visualize_time_func}
# Reference:
# + https://juliasilge.com/blog/ten-thousand-data-ext/.
visualize_time <-
  function(data,
           colname_x,
           geom = c("bar", "hist"),
           color = "grey50",
           lab_subtitle = NULL) {

    geom <- match.arg(geom)
    viz_labs <-
      labs(
        x = NULL,
        y = NULL,
        title = "Count Of Searches",
        subtitle = lab_subtitle
      )
    viz_theme <-
      temisc::theme_te_a() +
      theme(panel.grid.major.x = element_blank()) +
      theme(legend.position = "none")

    viz <- ggplot(data, aes_string(x = colname_x)) 
    if (geom == "bar") {
      viz <-
        viz +
        geom_bar(aes(y = ..count.., alpha = ..count..), fill = color) +
        scale_alpha(range = c(0.5, 1))
    } else if (geom == "hist") {
      viz <-
        viz +
        geom_histogram(aes(y = ..count..), fill = color, bins = 30)
    }

    viz <-
      viz +
      viz_labs +
      viz_theme
    viz
  }
```

Using this function is fairly straightforward. For example, to visualize the count
of searches by year, it can be invoked in the following manner.

```{r visualize_time_example, eval = FALSE}
viz_time_yyyy <-
  visualize_time(
    data = data,
    colname_x = "yyyy",
    geom = "bar",
    color = params$color_main,
    lab_subtitle = "By Year"
  )
```

The same pattern can be repeated for `timestamp`, `yyyy`, `mm`, `wd`, and `hh`.

```{r visualize_time_usage, echo = FALSE}
lab_subtitle_all <-
  paste0(
    "From ",
    strftime(data$timestamp[1], "%Y-%m-%d"),
    " to ",
    strftime(rev(data$timestamp)[1], "%Y-%m-%d")
  )

viz_time_all <-
  visualize_time(
    data = data,
    colname_x = "timestamp",
    geom = "hist",
    color = params$color_main,
    lab_subtitle = lab_subtitle_all
  )
viz_time_all
# NOTE: Could probably use `purrr::pmap()` for the following.
viz_time_yyyy <-
  visualize_time(
    data = data,
    colname_x = "yyyy",
    geom = "bar",
    color = params$color_main,
    lab_subtitle = "By Year"
  )
viz_time_yyyy 
viz_time_mm <-
  visualize_time(
    data = data,
    colname_x = "mm",
    geom = "bar",
    color = params$color_main,
    lab_subtitle = "By Month"
  )
viz_time_mm
viz_time_wd <-
  visualize_time(
    data = data,
    colname_x = "wd",
    geom = "bar",
    color = params$color_main,
    lab_subtitle = "By Day of Week"
  )
viz_time_wd
viz_time_hh <-
  visualize_time(
    data = data,
    colname_x = "hh",
    geom = "bar",
    color = params$color_main,
    lab_subtitle = "By Hour"
  )
viz_time_hh
```

I can make a couple of interesting observations about my data.

+ It's evident that I've googled stuff more and more frequently over the years.
+ It seems like my most active months correspond with typical American 
high school/college breaks--winter break occurs during December/January, 
spring break occurs in March, and the end of summer break occurs in August.
+ My relatively high activity on Saturdays and Sundays (compared to the rest of the days
of the week) indicate that I like to spend my "breaks" of weekly school/work on the Internet.
+ Regarding my hour-to-hour activity, mine seems relatively even throughout the day. 
I think if you compared my by-hour activity to others, mine would stand out as abnormally consistent.


```{r viz_time_unused, include = FALSE, eval = FALSE}
# Of course, other `geom`s can be used to visualize the data in other interesting ways.
# For instance, I could create a violin plot for the by-hour counts.
# Reference:
# + https://buzzfeednews.github.io/2018-01-trump-twitter-wars/
viz_time_hh_2 <-
  data %>%
  ggplot(aes(x = name, y = time, fill = name)) +
  scale_y_continuous(
    limits = c(1, 24),
    breaks = c(6, 12, 18),
    labels = c("6am", "Noon", "6pm")
  ) +
  scale_fill_manual(values = params$color_main) +
  geom_violin(size = 0) +
  geom_hline(
    yintercept = seq(3, 24, by = 3),
    color = "gray",
    size = 0.1
  ) +
  labs(x = NULL, y = NULL) +
  labs(title = "Count Over Time", lab_subtitle = "By Time of Day") +
  temisc::theme_te_a_dx() +
  theme(legend.position = "none", panel.grid = element_blank()) +
  coord_flip()
viz_time_hh_2
# Reference:
# + https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#
viz_time_hh_wd <-
  data %>%
  ggplot(aes(x = hh, group = wd, fill = name)) +
  geom_bar() +
  scale_fill_manual(values = params$color_main) +
  facet_wrap( ~ wd, scales = "free") +
  labs(x = NULL, y = NULL) +
  labs(title = "Count Over Time", lab_subtitle = "By Hour of Day and Time of Day") +
  temisc::theme_te_a_facet_dx() +
  theme(legend.position = "none", panel.grid = element_blank())
viz_time_hh_wd
# Reference:
# + https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#
viz_time_wd_yyyy <-
  data %>%
  count(name, yyyy, wd) %>%
  ggplot(aes(x = wd, y = n, fill = yyyy)) +
  geom_bar(stat = "identity") +
  facet_wrap(~name, scales = "free") +
  labs(x = NULL, y = NULL) +
  labs(title = "Count Over Time", lab_subtitle = "By Day of Week and Year") +
  temisc::theme_te_a_facet_dx() +
  theme(legend.position = "bottom", panel.grid = element_blank())
viz_time_wd_yyyy
```

### Word Frequencies

Now we'll "tokenize" the search text into n-grams. We'll parse each
search query into unigrams and bigrams.

```{r ngrams}
# Reference (for regular expression):
# + https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#
rgx_patt <- '(http|https)\\S+\\s*|(#|@)\\S+\\s*|\\n|\\"|(.*.)\\.com(.*.)\\S+\\s|[^[:alnum:]]'
rgx_repl <- " "
rgx_custom_ignore <- "google|search"

# References:
# + https://www.tidytextmining.com/
# + https://www.tidytextmining.com/ngrams.html
# + https://www.tidytextmining.com/twitter.html
stop_words <- tidytext::stop_words
unigrams <-
  data %>%
  mutate(text = str_replace_all(text, rgx_patt, rgx_repl)) %>% 
  tidytext::unnest_tokens(word, text) %>% 
  anti_join(stop_words, by = "word") %>% 
  filter(!str_detect(word, rgx_custom_ignore)) %>% 
  filter(str_detect(word, "[a-z]"))
unigrams %>% select(word) %>% count(word, sort = TRUE)

# References:
# + https://www.tidytextmining.com/
# + https://www.tidytextmining.com/ngrams.html
bigrams <-
  data %>%
  mutate(text = str_replace_all(text, rgx_patt, rgx_repl)) %>% 
  tidytext::unnest_tokens(word, text, token = "ngrams", n = 2) %>% 
  tidyr::separate(word, into = c("word1", "word2"), sep = " ", remove = FALSE) %>% 
  anti_join(stop_words, by = c("word1" = "word")) %>% 
  anti_join(stop_words, by = c("word2" = "word")) %>% 
  filter(!str_detect(word1, rgx_custom_ignore)) %>% 
  filter(!str_detect(word2, rgx_custom_ignore)) %>% 
  filter(str_detect(word1, "[a-z]")) %>% 
  filter(str_detect(word2, "[a-z]"))
bigrams %>% select(word) %>% count(word, sort = TRUE)
```

```{r ngrams_io, echo = FALSE, eval = FALSE}
filepath_unigrams <- "data-ext/unigrams-tony.rds"
filepath_bigrams <- "data-ext/bigrams-tony.rds"
# saveRDS(unigrams, file = filepath_unigrams)
# saveRDS(bigrams, file = filepath_bigrams)
unigrams <- readRDS(file = filepath_unigrams)
bigrams <- readRDS(file = filepath_bigrams)
```

With the data parsed into tokens, we can visualize counts of 
individual n-grams.

```{r viz_ngram_cnts}
# Reference:
# + https://github.com/dgrtwo/dgrtwo.github.com/blob/master/_R/2016-08-09-trump-data.Rmd.
visualize_cnts <- function(data, color = "grey50", num_top = 20) {
  data %>% 
    count(word, sort = TRUE) %>%
    filter(row_number(desc(n)) <= num_top) %>%
    mutate(word = reorder(word, n)) %>% 
    ggplot(aes(x = word, y = n)) +
    ggalt::geom_lollipop(size = 2, point.size = 4, color = color) +
    coord_flip() +
    temisc::theme_te_a() +
    labs(x = NULL, y = NULL) +
    labs(title = "Most Common Words") +
    theme(legend.position = "none") +
    theme(panel.grid.major.y = element_blank())
}

num_top_cnt <- 15
viz_unigram_cnts <-
  visualize_cnts(
    data = unigrams,
    color = params$color_main,
    num_top = num_top_cnt
  )
viz_unigram_cnts

viz_bigram_cnts <-
  visualize_cnts(
    data = bigrams,
    color = params$color_main,
    num_top = num_top_cnt
  )
viz_bigram_cnts
```

These count totals reflect my personal interests relatively well.
In particular, words like `nutrition` and `chicken breast`, 
`excel vba` and `python`, and
`nba` and `nfl scores` highlight my interest in
in food/nutrition, software and data analysis, and sports.
Additionally, the places I've lived re apparent from my searches--
`ut austin` reflects my undergraduate studies at
the [University of Texas at Austin](https://www.utexas.edu),
`baton rouge` alludes to my internship with 
[ExxonMobil](corporate.exxonmobil.com/)
in Baton Rouge in the summer of 2015,
`round rock` hints to my current residence in Round Rock, Texas.

#### Word Clouds

Another method of visualizing counts is with a word cloud.
Normally, I'm staunchly opposed to word clouds; however,
when used to initialize a mental model of the data, they're not so bad. 
I write a basic function so I can use it twice.

```{r visualize_cnts_wordcloud}
visualize_cnts_wordcloud <-
  function(data, color, num_top = 25) {
    data_proc <- data %>% count(word, sort = TRUE)
    wordcloud::wordcloud(
      word = data_proc$word,
      freq = data_proc$n,
      random.order = FALSE,
      colors = color,
      max.words = num_top
    )
  }
get_rpal_byname <- function(name) {
  paste0(name, c("", as.character(seq(1, 4, 1))))
}

colors_wordcloud <- get_rpal_byname(params$color_main)
num_top_cnt_wordcloud <- 25
visualize_cnts_wordcloud(
  data = unigrams,
  color = colors_wordcloud,
  num_top = num_top_cnt_wordcloud
)

visualize_cnts_wordcloud(
  data = bigrams,
  color = colors_wordcloud,
  num_top = num_top_cnt_wordcloud
)

```

These word clouds essentially show the same information as the other frequency plots,
so it's not surprising to see the same set of words shown. The word clouds
arguably do a better job of emphasizing the words themselves (as opposed
to the raw count totals associated with each word(s)).

```{r compute_freqs, include = FALSE}
compute_freqs <- function(data, colname_cnt = "word") {
  colname_cnt_quo <- rlang::sym(colname_cnt)
  
  data %>% 
    group_by(!!colname_cnt_quo) %>% 
    mutate(n = n()) %>%
    # ungroup() %>% 
    # group_by(!!colname_cnt_quo) %>% 
    summarize(freq = sum(n) / n()) %>% 
    ungroup() %>% 
    arrange(desc(freq))
}

unigrams_freqs <-
  compute_freqs(
    data = unigrams,
    colname_cnt = "word"
  )
unigrams_freqs

bigrams_freqs <-
  compute_freqs(
    data = bigrams,
    colname_cnt = "word"
  )
bigrams_freqs
```

### Word Correlations

Let's add a layer of complexity to our analysis. We can look at correlations
among individual words in each search. We'll create a fairly robust function here
because we'll need to perform the same actions twice--once to view the computed
values, and another time to put the data in the proper format for a network visualization.
(We need both the counts and the correlations of each word pair to create node-edge pairs.)

```{r compute_corrs_func}
# Reference:
# + https://www.tidytextmining.com/ngrams.html
# + http://varianceexplained.org/r/seven-fav-packages/.
compute_corrs <-
  function(data = NULL,
           colname_word = NULL,
           colname_feature = NULL,
           num_top_ngrams = 50,
           num_top_corrs = 50,
           return_corrs = TRUE,
           return_words = FALSE,
           return_both = FALSE) {

    colname_word_quo <- rlang::sym(colname_word)
    colname_feature_quo <- rlang::sym(colname_feature)
    
    data_cnt <-
      data %>%
      count(!!colname_word_quo, sort = TRUE)

    data_cnt_top <-
      data_cnt %>% 
      mutate(rank = row_number(desc(n))) %>% 
      filter(rank <= num_top_ngrams)
    
    data_joined <-
      data %>% 
      semi_join(data_cnt_top, by = colname_word) %>% 
      rename(
        word = !!colname_word_quo,
        feature = !!colname_feature_quo
      )
    data_corrs <-
      widyr::pairwise_cor(
        data_joined,
        word,
        feature,
        sort = TRUE,
        upper = FALSE
      )
    
    data_corrs_top <-
      data_corrs %>% 
      mutate(rank = row_number(desc(correlation))) %>% 
      filter(rank <= num_top_corrs)
    
    if(return_both | (return_words & return_corrs)) {
      out <- list(words = data_cnt_top, corrs = data_corrs_top)
    } else if (return_corrs) {
      out <- data_corrs_top
    } else if (return_words) {
      out <- data_cnt_top
    }
    out
  }

num_top_ngrams <- 50
num_top_corrs <- 50

unigrams_corrs <-
  compute_corrs(
    unigrams,
    num_top_ngrams = num_top_ngrams,
    num_top_corrs = num_top_corrs,
    colname_word = "word",
    colname_feature = "timestamp"
  )
unigrams_corrs
```

Not surprisingly, many of the same word pairs seen among the most frequently used
bigrams also appear here.

```{r viz_corrs_network}
# Reference:
# + http://varianceexplained.org/r/seven-fav-packages/.
unigrams_corrs_list <-
  compute_corrs(
    unigrams,
    num_top_ngrams = num_top_ngrams,
    num_top_corrs = num_top_corrs,
    colname_word = "word",
    colname_feature = "timestamp",
    return_both = TRUE
  )

seed <- 42
set.seed(seed)
viz_corrs_network <-
  igraph::graph_from_data_frame(
    d = unigrams_corrs_list$corrs, 
    vertices = unigrams_corrs_list$words, 
    directed = TRUE
  ) %>% 
  ggraph::ggraph(layout = "fr") +
  ggraph::geom_edge_link(edge_width = 1) +
  ggraph::geom_node_point(aes(size = n), fill = "grey50", shape = 21) +
  ggraph::geom_node_text(ggplot2::aes_string(label = "name"), repel = TRUE) +
  theme_te_a() +
  theme(line = element_blank(), rect = element_blank(), axis.text = element_blank(), axis.ticks = element_blank()) +
  theme(legend.position = "none") +
  labs(x = NULL, y = NULL) +
  labs(title = "Network of Pairwise Correlations", subtitle = "By Search")
viz_corrs_network
```

It looks like the network captures most of the similar terms fairly well.
The words for food/nutrition, software, and locations are grouped.

### Word Changes Over Time

We might be interested to find out whether certain words
have either been used more or less as time has passed.
Determining the highest changes in word usage is not quite as straightforward
as some of the other components of the text analysis so far.
There are various valid approaches that could be implemented. Here,
we'll
follow the approach shown in the
[Twitter chapter in the _Tidy Text Mining_ book](https://www.tidytextmining.com/twitter.html#changes-in-word-use).

First, we'll group my word usage by year and look only at the most used words.

```{r unigrams_bytime}
# Reference:
# + https://www.tidytextmining.com/twitter.html#changes-in-word-use.
timefloor <- "year"
top_pct_words <- 0.05
unigrams_bytime <-
  unigrams %>%
  mutate(time_floor = floor_date(timestamp, unit = timefloor)) %>%
  group_by(time_floor, word) %>% 
  summarise(n = n()) %>% 
  ungroup() %>%
  group_by(time_floor) %>%
  mutate(time_total = sum(n)) %>%
  ungroup() %>% 
  group_by(word) %>%
  mutate(word_total = sum(n)) %>%
  ungroup() %>%
  filter(word_total >= quantile(word_total, 1 - top_pct_words)) %>% 
  arrange(desc(word_total))
```

Next, we'll create logistic models for each word-year pair. These models attempt
essentially answer the question "How likely is it that a given word appears in a
given year?"

```{r unigrams_bytime_models}
unigrams_bytime_models <- 
  unigrams_bytime %>% 
  tidyr::nest(-word) %>% 
  mutate(
    models =
      purrr::map(data, ~ glm(cbind(n, time_total) ~ time_floor, ., family = "binomial"))
  )

unigrams_bytime_models_slopes <-
  unigrams_bytime_models %>%
  tidyr::unnest(purrr::map(models, broom::tidy)) %>%
  filter(term == "time_floor") %>%
  mutate(adjusted_p_value = p.adjust(p.value))
unigrams_bytime_models_slopes
```

The p.values of the logistic models indicate whether or not the change in usage
of a given word over time is non-trivial. We'll look at the words with the smallest
p.values, indicating that they have the most significant change in usage.

```{r viz_unigrams_change_bytime}
num_top_change <- 5
unigrams_bytime_models_slopes_top <-
  unigrams_bytime_models_slopes %>% 
  top_n(num_top_change, -adjusted_p_value) 

viz_unigrams_change_bytime <-
  unigrams_bytime %>% 
  inner_join(unigrams_bytime_models_slopes_top, by = c("word")) %>%
  mutate(pct = n / time_total) %>%
  mutate(label = if_else(time_floor == max(time_floor), word, NA_character_)) %>%
  ggplot(aes(x = time_floor, y = pct, color = word)) +
  geom_line(size = 1.5) +
  ggrepel::geom_label_repel(aes(label = label), nudge_x = 1, na.rm = TRUE) +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_color_func() +
  temisc::theme_te_a() +
  theme(legend.position = "none") +
  labs(x = NULL, y = NULL) +
  labs(title = "Largest Changes in Word Frequency")
viz_unigrams_change_bytime
```

This visualization does a great job of capturing how my interests/skills
have changed over time.

+ The rise of `excel` and `vba` recently reflect how much I have had
to develop my Excel VBA skills at work (after starting my job in mid-2016).
(I've actually had to enhance other software-related skills, such as with those with
`SQL`, `R`, and mapping software such as `AutoCAD`, but Excel's `VBA` has a lot of 
little nuances that make it not so intuitive (in my opinion) 
and put me more in need of Internet help than anything else.)
+ The steep decline in `python` from 2016 to 2017 illustrates how I learned python
early in 2016 as part of a "side-project", 
but then stopped my learning of it in favor of other languages/technologies that I need/use
for my job.
+ My interest in nutrition has waxed and waned over time. I think this is
probably because I learned a lot when reading about it for a couple of years, but now
have found myself less in need of researching because I know a good deal about it now.
+ The appearance of `ib` might be confusing to the reader. "IB" stands for
[International Baccalaureate](www.ibo.org/) (IB) which is a high school program for 
that is similar to th
[Advanced Placement](https://apstudent.collegeboard.org/apcourse) (AP) program that United States
high school students are probably more familiar with. After participating in the IB program
in high school, it is evident that my interest in it dropped off.


### Unique Words

#### Term-Frequency Inverse Document Frequency (TF-IDF) 

Another good way of evaluating my search behavior is to look at term-frequency
inverse-document-frequency (TF-IDF). I'll leave the reader to read the details
in the [_Tidy Text Mining_](https://www.tidytextmining.com/tfidf.html) book, 
but, in a nutshell, TF-IDF provides a good measure
of the most "unique" words in a given document compared to other documents.
For this analysis,
we'll treat the years of search history as documents.

```{r viz_tfidf, fig.width = 8, fig.height = 8}
# References:
# + https://www.tidytextmining.com/tfidf.html
# + https://juliasilge.com/blog/sherlock-holmes-stm/
data_tfidf <-
  unigrams %>%
  count(yyyy, word, sort = TRUE) %>% 
  tidytext::bind_tf_idf(word, yyyy, n)
data_tfidf

num_top_tfidf <- 10
viz_tfidf <-
  data_tfidf %>%
  group_by(yyyy) %>%
  # arrange(yyyy, desc(tf_idf)) %>%
  # slice(1:num_top_tfidf) %>%
  top_n(num_top_tfidf, tf_idf) %>% 
  ungroup() %>% 
  mutate(yyyy = factor(yyyy)) %>% 
  mutate(word = drlib::reorder_within(word, tf_idf, yyyy)) %>%
  ggplot(aes(word, tf_idf, fill = yyyy)) +
  geom_col() +
  scale_fill_func() +
  facet_wrap(~ yyyy, scales = "free") +
  drlib::scale_x_reordered() +
  coord_flip() +
  temisc::theme_te_a_dx() +
  theme(legend.position = "none") +
  theme(axis.text.x = element_blank()) +
  labs(x = NULL, y = NULL) +
  labs(title = "Highest TF-IDF Words", subtitle = "By Year")
viz_tfidf
```

This TF-IDF plot is probably my favorite one out of all of them. It really highlights
how my use of the Internet and personal interests have changed over time (perhaps even
better than the previous plot.

+ Up through graduating from high school in the middle of 2012, my search terms
don't appear correlated with anything in particular. This makes sense to me--at this point
in my life, I mostly used Google for doing research for school projects.
+ From the middle of 2012 to the middle of 2016, I was in college studying to get
a degree in Electrical Engineering. Words such as `integral`, `ut`, and `neutron`, reflect
my education. At the same time, my interest in health was strongest during these years,
so the food-related words are not surprising.
+ My more recent focus on software-related skills is evident in 2016 and beyond.
In particular, my growing passion for `R` in 2017 is illustrated with words such as
`ggplot`, `shiny`, `dplyr`, and `knitr`. This is one aspect of my personal skill
development that was not as evident in the previous plot.


## Conclusion

I'll continue this analysis in a separate write-up, where I plan to investigate
how "topic modeling" can be applied to gain further insight.
Topic modeling is much more dependent on the nature of the data 
than the analysis done here (which is fairly "generalizable"),
so I think it deserves distinct treatment.

Thanks again to David Robinson and Julia Silge for their great
[_Tidy Text Mining with R_ book](https://www.tidytextmining.com/)!
It demonstrates simple, yet powerful, techniques
that can be easily leveraged to gain meaningful insight into nearly anything you can imagine.


