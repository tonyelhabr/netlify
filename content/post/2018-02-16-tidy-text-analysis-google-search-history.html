---
title: A Tidy Text Analysis of My Google Search History
date: "2018-02-16"
slug: tidy-text-analysis-google-search-history
categories:
  - r
tags:
  - r
  - tidytext
  - ggplot2
  - dplyr
  - google
image:
  caption: ""
header:
  caption: ""
  image: "tidy-text-analysis-google-search-history/viz_corrs_network-tony-banner.png"
  preview: true
params:
  eval: false
---



<p>While brainstorming about cool ways to practice text mining with R I came up with the idea of exploring my own Google search history. Then, after googling (ironically) if anyone had done something like this, I stumbled upon <a href="https://lisacharlotterost.github.io/2015/06/20/Searching-through-the-years/">Lisa Charlotte’s blog post</a>. Lisa’s post (actually, a series of posts) are from a while back, so her instructions for how to download your personal Google history and the format of the downloads (nowadays, it’s in a .html file instead of a series of .json files) are no longer applicable.</p>
<p>I googled a bit more and found a recent <a href="https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#">RPubs write-up by Stephanie Lancz</a> that not only included concise instructions on how/where to get personal Google data, but also how to clean it with <code>R</code>! With the hard work of figuring out how to set up the data provided for me, I was excited to find out what I could do.</p>
<p>In this write-up (which can be downloaded from GitHub and re-used for one’s own analysis), I explore different techniques for visualizing and understanding my data. I do my best to implement methods that are generic and could be applied to any kind of similar analysis, irregardless of the topic. Much of my code is guided by the work of others, especially that of David Robinson and Julia Silge, who have written an amazingly helpful book on text analysis– <a href="https://www.tidytextmining.com/"><em>Tidy Text Mining with R</em> book</a>, I provide references for my inspiration where appropriate.</p>
<div id="setup" class="section level2">
<h2>Setup</h2>
<p>First, following “best practices”, I import all of the packages that I’ll be using.</p>
<pre class="r"><code>library(&quot;dplyr&quot;)
library(&quot;stringr&quot;)
library(&quot;xml2&quot;)
library(&quot;rvest&quot;)
library(&quot;lubridate&quot;)
library(&quot;viridis&quot;)
library(&quot;ggplot2&quot;)
library(&quot;tidytext&quot;)
library(&quot;tidyr&quot;)
library(&quot;ggalt&quot;)
library(&quot;widyr&quot;)
library(&quot;drlib&quot;)
library(&quot;igraph&quot;)
library(&quot;ggraph&quot;)
# library(&quot;topicmodels&quot;)
# devtools::install_github(&quot;tonyelhabr/temisc&quot;)
library(&quot;teplot&quot;) # Personal package.</code></pre>
<p>Next, I create a <code>config</code> list in order to emulate what one might do with a parameterized RMarkdown report (where the <code>config</code> would be a part of the yaml header.</p>
<pre class="r"><code>config &lt;-
  list(
    path = &quot;data-raw/Tony-My Activity-Search-MyActivity.html&quot;,
    name_main = &quot;Tony&quot;,
    color_main = &quot;firebrick&quot;
  )</code></pre>
<p>I’ll also go ahead and create a couple of functions for coloring some of the plots that I’ll create. These can be customized to one’s personal preferences.</p>
<pre class="r"><code>scale_color_func &lt;- function() {
  viridis::scale_color_viridis(
    option = &quot;D&quot;,
    discrete = TRUE,
    begin = 0,
    end = 0.75
  ) 
}

scale_fill_func &lt;- function() {
  viridis::scale_fill_viridis(
    option = &quot;D&quot;,
    discrete = TRUE,
    begin = 0,
    end = 0.75
  ) 
}</code></pre>
<div id="import-and-clean" class="section level3">
<h3>Import and Clean</h3>
<p>Then, on to the “dirty” work of importing and cleaning the data. I don’t deviate much from <a href="https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#">Stephanie Lancz’s methods</a> for extracting data elements from the .html file</p>
<pre class="r"><code># Reference:
# + https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#

doc_html &lt;- config$path
search_archive &lt;- xml2::read_html(doc_html)

# Extract search time.
date_search &lt;-
  search_archive %&gt;%
  html_nodes(xpath = &#39;//div[@class=&quot;mdl-grid&quot;]/div/div&#39;) %&gt;%
  str_extract(pattern = &quot;(?&lt;=&lt;br&gt;)(.*)(?&lt;=PM|AM)&quot;) %&gt;%
  mdy_hms()

# Extract search text.
text_search &lt;-
  search_archive %&gt;%
  html_nodes(xpath = &#39;//div[@class=&quot;mdl-grid&quot;]/div/div&#39;) %&gt;%
  str_extract(pattern = &#39;(?&lt;=&lt;a)(.*)(?=&lt;/a&gt;)&#39;) %&gt;%
  str_extract(pattern = &#39;(?&lt;=\&quot;&gt;)(.*)&#39;)

# Extract search type.
type_search &lt;-
  search_archive %&gt;% 
  html_nodes(xpath = &#39;//div[@class=&quot;mdl-grid&quot;]/div/div&#39;) %&gt;% 
  str_extract(pattern = &quot;(?&lt;=mdl-typography--body-1\&quot;&gt;)(.*)(?=&lt;a)&quot;) %&gt;% 
  str_extract(pattern = &quot;(\\w+)(?=\\s)&quot;)

# Differences from reference:
# + Using `lubridate::wday()` instead of calling `weekdays()` and coercing to factors.
# + Using `yyyy`, `mm`, `wd`, and `hh` instead of `year`, `month`, `wday`, and `hour`.
# + Convert `yyyy` to an integer (from a double).
# + Adding a `time` column to use for a later visualization.
# + Adding a `name` column to make this code more &quot;parametric&quot;.
data &lt;-
  tibble(
    name = config$name_main,
    timestamp = date_search,
    date = lubridate::as_date(date_search),
    yyyy = lubridate::year(date_search) %&gt;% as.integer(),
    mm = lubridate::month(date_search, label = TRUE),
    wd = lubridate::wday(date_search, label = TRUE),
    hh = lubridate::hour(date_search),
    time = lubridate::hour(timestamp) + (lubridate::minute(timestamp) / 60),
    type = type_search,
    text = text_search
  )</code></pre>
<p>Notably, there are some rows that did not get parsed correctly. I decide to exclude them from the rest of the analysis. Also, my first searches come at the end of 2010, and my most recent ones (i.e. the ones just before I downloaded my data) come in the first month or so of 2018. To make the aspect of my analysis that deal with years a bit “cleaner”, I’ll truncate these ends so that my data spans the years 2011 through 2017.</p>
<pre class="r"><code>data %&gt;% count(yyyy, sort = TRUE)
data &lt;- data %&gt;% filter(!is.na(yyyy))
data &lt;- data %&gt;% filter(!(yyyy %in% c(2010, 2018)))</code></pre>
</div>
</div>
<div id="analysis" class="section level2">
<h2>Analysis</h2>
<div id="search-count-distributions" class="section level3">
<h3>Search Count Distributions</h3>
<p>Next, it’s time to start doing some basic exploratory data analysis (EDA). Given the temporal nature of the data, an easy EDA approach to implement is visualization across different time periods. To save some effort (or, as I like to see it, make my code more efficient), we can create a helper function. (Notably, the <code>geom</code> to use is a parameter to this function. Through experimentation, I found that <code>ggplot2::geom_bar()</code> seems to work best with most temporal periods, with the exception of plotting <code>Date</code> variables, where <code>ggplot2::geom_hist()</code> seems more appropriate.)</p>
<pre class="r"><code># Reference:
# + https://juliasilge.com/blog/ten-thousand-data-ext/.
visualize_time &lt;-
  function(data,
           colname_x,
           geom = c(&quot;bar&quot;, &quot;hist&quot;),
           color = &quot;grey50&quot;,
           lab_subtitle = NULL) {

    geom &lt;- match.arg(geom)
    viz_labs &lt;-
      labs(
        x = NULL,
        y = NULL,
        title = &quot;Count Of Searches&quot;,
        subtitle = lab_subtitle
      )
    viz_theme &lt;-
      teplot::theme_te() +
      theme(panel.grid.major.x = element_blank()) +
      theme(legend.position = &quot;none&quot;)

    viz &lt;- ggplot(data, aes_string(x = colname_x)) 
    if (geom == &quot;bar&quot;) {
      viz &lt;-
        viz +
        geom_bar(aes(y = ..count.., alpha = ..count..), fill = color) +
        scale_alpha(range = c(0.5, 1))
    } else if (geom == &quot;hist&quot;) {
      viz &lt;-
        viz +
        geom_histogram(aes(y = ..count..), fill = color, bins = 30)
    }

    viz &lt;-
      viz +
      viz_labs +
      viz_theme
    viz
  }</code></pre>
<p>Using this function is fairly straightforward. For example, to visualize the count of searches by year, it can be invoked in the following manner.</p>
<pre class="r"><code>viz_time_yyyy &lt;-
  visualize_time(
    data = data,
    colname_x = &quot;yyyy&quot;,
    geom = &quot;bar&quot;,
    color = config$color_main,
    lab_subtitle = &quot;By Year&quot;
  )</code></pre>
<p>The same pattern can be repeated for <code>timestamp</code>, <code>yyyy</code>, <code>mm</code>, <code>wd</code>, and <code>hh</code>.</p>
<p><img src="/post/2018-02-16-tidy-text-analysis-google-search-history_files/figure-html/viz_time_show-1.png" width="576" style="display: block; margin: auto;" /><img src="/post/2018-02-16-tidy-text-analysis-google-search-history_files/figure-html/viz_time_show-2.png" width="576" style="display: block; margin: auto;" /><img src="/post/2018-02-16-tidy-text-analysis-google-search-history_files/figure-html/viz_time_show-3.png" width="576" style="display: block; margin: auto;" /><img src="/post/2018-02-16-tidy-text-analysis-google-search-history_files/figure-html/viz_time_show-4.png" width="576" style="display: block; margin: auto;" /><img src="/post/2018-02-16-tidy-text-analysis-google-search-history_files/figure-html/viz_time_show-5.png" width="576" style="display: block; margin: auto;" /></p>
<p>I can make a couple of interesting observations about my data.</p>
<ul>
<li>It’s evident that I’ve googled stuff more and more frequently over the years.</li>
<li>It seems like my most active months correspond with typical American high school/college breaks–winter break occurs during December/January, spring break occurs in March, and the end of summer break occurs in August.</li>
<li>My relatively high activity on Saturdays and Sundays (compared to the rest of the days of the week) indicate that I like to spend my “breaks” of weekly school/work on the Internet.</li>
<li>Regarding my hour-to-hour activity, mine seems relatively even throughout the day. I think if you compared my by-hour activity to others, mine would stand out as abnormally consistent.</li>
</ul>
</div>
<div id="word-frequencies" class="section level3">
<h3>Word Frequencies</h3>
<p>Now we’ll “tokenize” the search text into n-grams. We’ll parse each search query into unigrams and bigrams.</p>
<pre class="r"><code># Reference (for regular expression):
# + https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#
rgx_patt &lt;- &#39;(http|https)\\S+\\s*|(#|@)\\S+\\s*|\\n|\\&quot;|(.*.)\\.com(.*.)\\S+\\s|[^[:alnum:]]&#39;
rgx_repl &lt;- &quot; &quot;
rgx_custom_ignore &lt;- &quot;google|search&quot;

# References:
# + https://www.tidytextmining.com/
# + https://www.tidytextmining.com/ngrams.html
# + https://www.tidytextmining.com/twitter.html
stop_words &lt;- tidytext::stop_words
unigrams &lt;-
  data %&gt;%
  mutate(text = str_replace_all(text, rgx_patt, rgx_repl)) %&gt;% 
  tidytext::unnest_tokens(word, text) %&gt;% 
  anti_join(stop_words, by = &quot;word&quot;) %&gt;% 
  filter(!str_detect(word, rgx_custom_ignore)) %&gt;% 
  filter(str_detect(word, &quot;[a-z]&quot;))
unigrams %&gt;% select(word) %&gt;% count(word, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 15,299 x 2
##    word          n
##    &lt;chr&gt;     &lt;int&gt;
##  1 nutrition  1677
##  2 excel      1224
##  3 ut          979
##  4 austin      811
##  5 vba         683
##  6 python      551
##  7 chicken     486
##  8 sql         453
##  9 nba         404
## 10 oracle      389
## # ... with 1.529e+04 more rows</code></pre>
<pre class="r"><code># References:
# + https://www.tidytextmining.com/
# + https://www.tidytextmining.com/ngrams.html
bigrams &lt;-
  data %&gt;%
  mutate(text = str_replace_all(text, rgx_patt, rgx_repl)) %&gt;% 
  tidytext::unnest_tokens(word, text, token = &quot;ngrams&quot;, n = 2) %&gt;% 
  tidyr::separate(word, into = c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;, remove = FALSE) %&gt;% 
  anti_join(stop_words, by = c(&quot;word1&quot; = &quot;word&quot;)) %&gt;% 
  anti_join(stop_words, by = c(&quot;word2&quot; = &quot;word&quot;)) %&gt;% 
  filter(!str_detect(word1, rgx_custom_ignore)) %&gt;% 
  filter(!str_detect(word2, rgx_custom_ignore)) %&gt;% 
  filter(str_detect(word1, &quot;[a-z]&quot;)) %&gt;% 
  filter(str_detect(word2, &quot;[a-z]&quot;))
bigrams %&gt;% select(word) %&gt;% count(word, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 33,404 x 2
##    word               n
##    &lt;chr&gt;          &lt;int&gt;
##  1 excel vba        598
##  2 ut austin        474
##  3 pl sql           167
##  4 san antonio      126
##  5 baton rouge      113
##  6 peanut butter    102
##  7 round rock       100
##  8 sweet potato      95
##  9 oracle sql        94
## 10 chicken breast    88
## # ... with 3.339e+04 more rows</code></pre>
<p>With the data parsed into tokens, we can visualize counts of individual n-grams.</p>
<pre class="r"><code># Reference:
# + https://github.com/dgrtwo/dgrtwo.github.com/blob/master/_R/2016-08-09-trump-data.Rmd.
visualize_cnts &lt;- function(data, color = &quot;grey50&quot;, num_top = 20) {
  data %&gt;% 
    count(word, sort = TRUE) %&gt;%
    filter(row_number(desc(n)) &lt;= num_top) %&gt;%
    mutate(word = reorder(word, n)) %&gt;% 
    ggplot(aes(x = word, y = n)) +
    ggalt::geom_lollipop(size = 2, point.size = 4, color = color) +
    coord_flip() +
    teplot::theme_te() +
    labs(x = NULL, y = NULL) +
    labs(title = &quot;Most Common Words&quot;) +
    theme(legend.position = &quot;none&quot;) +
    theme(panel.grid.major.y = element_blank())
}

num_top_cnt &lt;- 15
viz_unigram_cnts &lt;-
  visualize_cnts(
    data = unigrams,
    color = config$color_main,
    num_top = num_top_cnt
  )
viz_unigram_cnts</code></pre>
<p><img src="/post/2018-02-16-tidy-text-analysis-google-search-history_files/figure-html/viz_unigram_cnts_show-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>viz_bigram_cnts &lt;-
  visualize_cnts(
    data = bigrams,
    color = config$color_main,
    num_top = num_top_cnt
  )
viz_bigram_cnts</code></pre>
<p><img src="/post/2018-02-16-tidy-text-analysis-google-search-history_files/figure-html/viz_bigram_cnts_show-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>These count totals reflect my personal interests relatively well. In particular, words like <code>nutrition</code> and <code>chicken breast</code>, <code>excel vba</code> and <code>python</code>, and <code>nba</code> and <code>nfl scores</code> highlight my interest in in food/nutrition, software and data analysis, and sports. Additionally, the places I’ve lived re apparent from my searches– <code>ut austin</code> reflects my undergraduate studies at the <a href="https://www.utexas.edu">University of Texas at Austin</a>, <code>baton rouge</code> alludes to my internship with <a href="corporate.exxonmobil.com/">ExxonMobil</a> in Baton Rouge in the summer of 2015, <code>round rock</code> hints to my current residence in Round Rock, Texas.</p>
<div id="word-clouds" class="section level4">
<h4>Word Clouds</h4>
<p>Another method of visualizing counts is with a word cloud. Normally, I’m staunchly opposed to word clouds; however, when used to initialize a mental model of the data, they’re not so bad. I write a basic function so I can use it twice.</p>
<pre class="r"><code>visualize_cnts_wordcloud &lt;-
  function(data, color, num_top = 25) {
    data_proc &lt;- data %&gt;% count(word, sort = TRUE)
    wordcloud::wordcloud(
      word = data_proc$word,
      freq = data_proc$n,
      random.order = FALSE,
      colors = color,
      max.words = num_top
    )
  }
get_rpal_byname &lt;- function(name) {
  paste0(name, c(&quot;&quot;, as.character(seq(1, 4, 1))))
}

colors_wordcloud &lt;- get_rpal_byname(config$color_main)
num_top_cnt_wordcloud &lt;- 25
viz_unigram_cnts_wordcloud &lt;-
  visualize_cnts_wordcloud(
    data = unigrams,
    color = colors_wordcloud,
    num_top = num_top_cnt_wordcloud
  )</code></pre>
<pre><code>## NULL</code></pre>
<pre class="r"><code>viz_bigram_cnts_wordcloud &lt;-
  visualize_cnts_wordcloud(
    data = bigrams,
    color = colors_wordcloud,
    num_top = num_top_cnt_wordcloud
  )</code></pre>
<pre><code>## NULL</code></pre>
<p>These word clouds essentially show the same information as the other frequency plots, so it’s not surprising to see the same set of words shown. The word clouds arguably do a better job of emphasizing the words themselves (as opposed to the raw count totals associated with each word(s)).</p>
<pre class="r"><code>compute_freqs &lt;- function(data, colname_cnt = &quot;word&quot;) {
  colname_cnt_quo &lt;- rlang::sym(colname_cnt)
  
  data %&gt;% 
    group_by(!!colname_cnt_quo) %&gt;% 
    mutate(n = n()) %&gt;%
    # ungroup() %&gt;% 
    # group_by(!!colname_cnt_quo) %&gt;% 
    summarize(freq = sum(n) / n()) %&gt;% 
    ungroup() %&gt;% 
    arrange(desc(freq))
}

unigram_freqs &lt;-
  compute_freqs(
    data = unigrams,
    colname_cnt = &quot;word&quot;
  )
unigram_freqs</code></pre>
<pre><code>## # A tibble: 15,299 x 2
##    word       freq
##    &lt;chr&gt;     &lt;dbl&gt;
##  1 nutrition  1677
##  2 excel      1224
##  3 ut          979
##  4 austin      811
##  5 vba         683
##  6 python      551
##  7 chicken     486
##  8 sql         453
##  9 nba         404
## 10 oracle      389
## # ... with 1.529e+04 more rows</code></pre>
<pre class="r"><code>bigram_freqs &lt;-
  compute_freqs(
    data = bigrams,
    colname_cnt = &quot;word&quot;
  )
bigram_freqs</code></pre>
<pre><code>## # A tibble: 33,404 x 2
##    word            freq
##    &lt;chr&gt;          &lt;dbl&gt;
##  1 excel vba        598
##  2 ut austin        474
##  3 pl sql           167
##  4 san antonio      126
##  5 baton rouge      113
##  6 peanut butter    102
##  7 round rock       100
##  8 sweet potato      95
##  9 oracle sql        94
## 10 chicken breast    88
## # ... with 3.339e+04 more rows</code></pre>
</div>
</div>
<div id="word-correlations" class="section level3">
<h3>Word Correlations</h3>
<p>Let’s add a layer of complexity to our analysis. We can look at correlations among individual words in each search. We’ll create a fairly robust function here because we’ll need to perform the same actions twice–once to view the computed values, and another time to put the data in the proper format for a network visualization. (We need both the counts and the correlations of each word pair to create node-edge pairs.)</p>
<pre class="r"><code># Reference:
# + https://www.tidytextmining.com/ngrams.html
# + http://varianceexplained.org/r/seven-fav-packages/.
compute_corrs &lt;-
  function(data = NULL,
           colname_word = NULL,
           colname_feature = NULL,
           num_top_ngrams = 50,
           num_top_corrs = 50,
           return_corrs = TRUE,
           return_words = FALSE,
           return_both = FALSE) {

    colname_word_quo &lt;- rlang::sym(colname_word)
    colname_feature_quo &lt;- rlang::sym(colname_feature)
    
    data_cnt &lt;-
      data %&gt;%
      count(!!colname_word_quo, sort = TRUE)

    data_cnt_top &lt;-
      data_cnt %&gt;% 
      mutate(rank = row_number(desc(n))) %&gt;% 
      filter(rank &lt;= num_top_ngrams)
    
    data_joined &lt;-
      data %&gt;% 
      semi_join(data_cnt_top, by = colname_word) %&gt;% 
      rename(
        word = !!colname_word_quo,
        feature = !!colname_feature_quo
      )
    data_corrs &lt;-
      widyr::pairwise_cor(
        data_joined,
        word,
        feature,
        sort = TRUE,
        upper = FALSE
      )
    
    data_corrs_top &lt;-
      data_corrs %&gt;% 
      mutate(rank = row_number(desc(correlation))) %&gt;% 
      filter(rank &lt;= num_top_corrs)
    
    if(return_both | (return_words &amp; return_corrs)) {
      out &lt;- list(words = data_cnt_top, corrs = data_corrs_top)
    } else if (return_corrs) {
      out &lt;- data_corrs_top
    } else if (return_words) {
      out &lt;- data_cnt_top
    }
    out
  }

num_top_ngrams &lt;- 50
num_top_corrs &lt;- 50

unigram_corrs &lt;-
  compute_corrs(
    unigrams,
    num_top_ngrams = num_top_ngrams,
    num_top_corrs = num_top_corrs,
    colname_word = &quot;word&quot;,
    colname_feature = &quot;timestamp&quot;
  )
unigram_corrs</code></pre>
<p>Not surprisingly, many of the same word pairs seen among the most frequently used bigrams also appear here.</p>
<pre class="r"><code># Reference:
# + http://varianceexplained.org/r/seven-fav-packages/.
unigram_corrs_list &lt;-
  compute_corrs(
    unigrams,
    num_top_ngrams = num_top_ngrams,
    num_top_corrs = num_top_corrs,
    colname_word = &quot;word&quot;,
    colname_feature = &quot;timestamp&quot;,
    return_both = TRUE
  )

seed &lt;- 42
set.seed(seed)
viz_corrs_network &lt;-
  igraph::graph_from_data_frame(
    d = unigram_corrs_list$corrs, 
    vertices = unigram_corrs_list$words, 
    directed = TRUE
  ) %&gt;% 
  ggraph::ggraph(layout = &quot;fr&quot;) +
  ggraph::geom_edge_link(edge_width = 1) +
  ggraph::geom_node_point(aes(size = n), fill = &quot;grey50&quot;, shape = 21) +
  ggraph::geom_node_text(ggplot2::aes_string(label = &quot;name&quot;), repel = TRUE) +
  teplot::theme_te() +
  theme(line = element_blank(), rect = element_blank(), axis.text = element_blank(), axis.ticks = element_blank()) +
  theme(legend.position = &quot;none&quot;) +
  labs(x = NULL, y = NULL) +
  labs(title = &quot;Network of Pairwise Correlations&quot;, subtitle = &quot;By Search&quot;)
viz_corrs_network</code></pre>
<p><img src="/post/2018-02-16-tidy-text-analysis-google-search-history_files/figure-html/viz_corrs_network_show-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>It looks like the network captures most of the similar terms fairly well. The words for food/nutrition, software, and locations are grouped.</p>
</div>
<div id="word-changes-over-time" class="section level3">
<h3>Word Changes Over Time</h3>
<p>We might be interested to find out whether certain words have either been used more or less as time has passed. Determining the highest changes in word usage is not quite as straightforward as some of the other components of the text analysis so far. There are various valid approaches that could be implemented. Here, we’ll follow the approach shown in the <a href="https://www.tidytextmining.com/twitter.html#changes-in-word-use">Twitter chapter in the <em>Tidy Text Mining</em> book</a>.</p>
<p>First, we’ll group my word usage by year and look only at the most used words.</p>
<pre class="r"><code># Reference:
# + https://www.tidytextmining.com/twitter.html#changes-in-word-use.
timefloor &lt;- &quot;year&quot;
top_pct_words &lt;- 0.05
unigram_bytime &lt;-
  unigrams %&gt;%
  mutate(time_floor = floor_date(timestamp, unit = timefloor)) %&gt;%
  group_by(time_floor, word) %&gt;% 
  summarise(n = n()) %&gt;% 
  ungroup() %&gt;%
  group_by(time_floor) %&gt;%
  mutate(time_total = sum(n)) %&gt;%
  ungroup() %&gt;% 
  group_by(word) %&gt;%
  mutate(word_total = sum(n)) %&gt;%
  ungroup() %&gt;%
  filter(word_total &gt;= quantile(word_total, 1 - top_pct_words)) %&gt;% 
  arrange(desc(word_total))
unigram_bytime</code></pre>
<pre><code>## # A tibble: 1,377 x 5
##    time_floor          word          n time_total word_total
##    &lt;dttm&gt;              &lt;chr&gt;     &lt;int&gt;      &lt;int&gt;      &lt;int&gt;
##  1 2012-01-01 00:00:00 nutrition    63       5683       1677
##  2 2013-01-01 00:00:00 nutrition   263      13380       1677
##  3 2014-01-01 00:00:00 nutrition   647      18824       1677
##  4 2015-01-01 00:00:00 nutrition   388      17202       1677
##  5 2016-01-01 00:00:00 nutrition   215      28586       1677
##  6 2017-01-01 00:00:00 nutrition   101      31681       1677
##  7 2013-01-01 00:00:00 excel         2      13380       1224
##  8 2014-01-01 00:00:00 excel       111      18824       1224
##  9 2015-01-01 00:00:00 excel        80      17202       1224
## 10 2016-01-01 00:00:00 excel       307      28586       1224
## # ... with 1,367 more rows</code></pre>
<p>Next, we’ll create logistic models for each word-year pair. These models attempt essentially answer the question “How likely is it that a given word appears in a given year?”</p>
<pre class="r"><code>unigram_bytime_models &lt;- 
  unigram_bytime %&gt;% 
  tidyr::nest(-word) %&gt;% 
  mutate(
    models =
      purrr::map(data, ~ glm(cbind(n, time_total) ~ time_floor, ., family = &quot;binomial&quot;))
  )

unigram_bytime_models_slopes &lt;-
  unigram_bytime_models %&gt;%
  tidyr::unnest(purrr::map(models, broom::tidy)) %&gt;%
  filter(term == &quot;time_floor&quot;) %&gt;%
  mutate(adjusted_p_value = p.adjust(p.value))
unigram_bytime_models_slopes</code></pre>
<pre><code>## # A tibble: 255 x 7
##    word   term    estimate std.error statistic    p.value adjusted_p_value
##    &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;            &lt;dbl&gt;
##  1 nutri~ time_~ -1.033e-8 4.891e-10   -21.12  5.093e- 99       1.294e- 96
##  2 excel  time_~  2.060e-8 9.561e-10    21.54  6.326e-103       1.613e-100
##  3 ut     time_~ -9.729e-9 6.339e-10   -15.35  3.748e- 53       9.370e- 51
##  4 austin time_~ -4.307e-9 7.014e-10    -6.141 8.197e- 10       1.476e-  7
##  5 vba    time_~  3.471e-8 1.892e- 9    18.35  3.415e- 75       8.641e- 73
##  6 python time_~ -7.032e-8 4.364e- 9   -16.11  2.092e- 58       5.250e- 56
##  7 chick~ time_~ -1.010e-8 8.992e-10   -11.24  2.708e- 29       6.634e- 27
##  8 sql    time_~  3.082e-8 2.148e- 9    14.35  1.098e- 46       2.735e- 44
##  9 nba    time_~  1.522e-8 1.392e- 9    10.93  8.010e- 28       1.947e- 25
## 10 oracle time_~  2.175e-8 2.042e- 9    10.65  1.727e- 26       4.129e- 24
## # ... with 245 more rows</code></pre>
<p>The p.values of the logistic models indicate whether or not the change in usage of a given word over time is non-trivial. We’ll look at the words with the smallest p.values, indicating that they have the most significant change in usage.</p>
<pre class="r"><code>num_top_change &lt;- 5
unigram_bytime_models_slopes_top &lt;-
  unigram_bytime_models_slopes %&gt;% 
  top_n(num_top_change, -adjusted_p_value) 

viz_unigram_change_bytime &lt;-
  unigram_bytime %&gt;% 
  inner_join(unigram_bytime_models_slopes_top, by = c(&quot;word&quot;)) %&gt;%
  mutate(pct = n / time_total) %&gt;%
  mutate(label = if_else(time_floor == max(time_floor), word, NA_character_)) %&gt;%
  ggplot(aes(x = time_floor, y = pct, color = word)) +
  geom_line(size = 1.5) +
  ggrepel::geom_label_repel(aes(label = label), nudge_x = 1, na.rm = TRUE) +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_color_func() +
  teplot::theme_te() +
  theme(legend.position = &quot;none&quot;) +
  labs(x = NULL, y = NULL) +
  labs(title = &quot;Largest Changes in Word Frequency&quot;)
viz_unigram_change_bytime</code></pre>
<p><img src="/post/2018-02-16-tidy-text-analysis-google-search-history_files/figure-html/viz_unigram_change_bytime_show-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>This visualization does a great job of capturing how my interests/skills have changed over time.</p>
<ul>
<li>The rise of <code>excel</code> and <code>vba</code> recently reflect how much I have had to develop my Excel VBA skills at work (after starting my job in mid-2016). (I’ve actually had to enhance other software-related skills, such as with those with <code>SQL</code>, <code>R</code>, and mapping software such as <code>AutoCAD</code>, but Excel’s <code>VBA</code> has a lot of little nuances that make it not so intuitive (in my opinion) and put me more in need of Internet help than anything else.)</li>
<li>The steep decline in <code>python</code> from 2016 to 2017 illustrates how I learned python early in 2016 as part of a “side-project”, but then stopped my learning of it in favor of other languages/technologies that I need/use for my job.</li>
<li>My interest in nutrition has waxed and waned over time. I think this is probably because I learned a lot when reading about it for a couple of years, but now have found myself less in need of researching because I know a good deal about it now.</li>
<li>The appearance of <code>ib</code> might be confusing to the reader. “IB” stands for <a href="www.ibo.org/">International Baccalaureate</a> (IB) which is a high school program for that is similar to th <a href="https://apstudent.collegeboard.org/apcourse">Advanced Placement</a> (AP) program that United States high school students are probably more familiar with. After participating in the IB program in high school, it is evident that my interest in it dropped off.</li>
</ul>
</div>
<div id="unique-words" class="section level3">
<h3>Unique Words</h3>
<div id="term-frequency-inverse-document-frequency-tf-idf" class="section level4">
<h4>Term-Frequency Inverse Document Frequency (TF-IDF)</h4>
<p>Another good way of evaluating my search behavior is to look at term-frequency inverse-document-frequency (TF-IDF). I’ll leave the reader to read the details in the <a href="https://www.tidytextmining.com/tfidf.html"><em>Tidy Text Mining</em></a> book, but, in a nutshell, TF-IDF provides a good measure of the most “unique” words in a given document compared to other documents. For this analysis, we’ll treat the years of search history as documents.</p>
<pre class="r"><code># References:
# + https://www.tidytextmining.com/tfidf.html
# + https://juliasilge.com/blog/sherlock-holmes-stm/
data_tfidf &lt;-
  unigrams %&gt;%
  count(yyyy, word, sort = TRUE) %&gt;% 
  tidytext::bind_tf_idf(word, yyyy, n)

num_top_tfidf &lt;- 10
viz_tfidf &lt;-
  data_tfidf %&gt;%
  group_by(yyyy) %&gt;%
  # arrange(yyyy, desc(tf_idf)) %&gt;%
  # slice(1:num_top_tfidf) %&gt;%
  top_n(num_top_tfidf, tf_idf) %&gt;% 
  ungroup() %&gt;% 
  mutate(yyyy = factor(yyyy)) %&gt;% 
  mutate(word = drlib::reorder_within(word, tf_idf, yyyy)) %&gt;%
  ggplot(aes(word, tf_idf, fill = yyyy)) +
  geom_col() +
  scale_fill_func() +
  facet_wrap(~ yyyy, scales = &quot;free&quot;) +
  drlib::scale_x_reordered() +
  coord_flip() +
  teplot::theme_te_dx() +
  theme(legend.position = &quot;none&quot;) +
  theme(axis.text.x = element_blank()) +
  labs(x = NULL, y = NULL) +
  labs(title = &quot;Highest TF-IDF Words&quot;, subtitle = &quot;By Year&quot;)
viz_tfidf</code></pre>
<p><img src="/post/2018-02-16-tidy-text-analysis-google-search-history_files/figure-html/viz_tfidf_show-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>This TF-IDF plot is probably my favorite one out of all of them. It really highlights how my use of the Internet and personal interests have changed over time (perhaps even better than the previous plot.</p>
<ul>
<li>Up through graduating from high school in the middle of 2012, my search terms don’t appear correlated with anything in particular. This makes sense to me–at this point in my life, I mostly used Google for doing research for school projects.</li>
<li>From the middle of 2012 to the middle of 2016, I was in college studying to get a degree in Electrical Engineering. Words such as <code>integral</code>, <code>ut</code>, and <code>neutron</code>, reflect my education. At the same time, my interest in health was strongest during these years, so the food-related words are not surprising.</li>
<li>My more recent focus on software-related skills is evident in 2016 and beyond. In particular, my growing passion for <code>R</code> in 2017 is illustrated with words such as <code>{ggplot}</code>, <code>{shiny}</code>, <code>{dplyr}</code>, and <code>{knitr}</code>. This is one aspect of my personal skill development that was not as evident in the previous plot.</li>
</ul>
</div>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>I’ll continue this analysis in a separate write-up, where I plan to investigate how “topic modeling” can be applied to gain further insight. Topic modeling is much more dependent on the nature of the data than the analysis done here (which is fairly “generalizable”), so I think it deserves distinct treatment.</p>
<p>Thanks again to David Robinson and Julia Silge for their great <a href="https://www.tidytextmining.com/"><em>Tidy Text Mining with R</em> book</a>! It demonstrates simple, yet powerful, techniques that can be easily leveraged to gain meaningful insight into nearly anything you can imagine.</p>
</div>
