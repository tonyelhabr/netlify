---
title: The Split-Apply-Combine Technique for Machine Learning with R
slug: split-apply-combine-machine-learning-r
date: "2018-08-05"
categories: []
tags:
 - r
 - tidyverse
 - caret
 - machine learning
banner: "img/split-apply-combine-machine-learning-r/purrrify_caret-banner.png"
---

```{r setup, include = FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
knitr::opts_chunk$set(
  echo = TRUE,
  # cache = TRUE,
  cache = FALSE,
  include = TRUE,
  fig.align = "center",
  # results = "asis",
  fig.width = 6,
  fig.height = 6,
  # out.width = 6,
  # out.height = 6,
  warning = FALSE,
  message = FALSE
)
options(scipen = 1, digits = 2)
```

```{r config, include = FALSE}
# NOTE TO SELF: Set export logicals to FALSE for blog post.
config <-
  list(
    export_data = FALSE,
    dir_data = file.path("static", "data"),
    export_viz = FALSE,
    dir_viz = "figs"
  )
```

```{r path_save, include = FALSE}
path_save <-
  file.path(config$dir_data, "purrrify_caret.RData")
```

```{r load, include = FALSE}
# NOTE TO SELF: Load only for blog post.
load(path_save)
```

## Introduction

Much discussion in the R community has revolved around the proper way
to implement the ["split-apply-combine"](https://www.google.com/search?q=split+apply+combine&rlz=1C1GGRV_enUS751US752&oq=split+apply+combine&aqs=chrome..69i57j69i60l2.2919j0j4&sourceid=chrome&ie=UTF-8).
In particular, I love the exploration of this topic 
[in this blog post](https://coolbutuseless.bitbucket.io/2018/03/03/split-apply-combine-my-search-for-a-replacement-for-group_by---do/).
It seems that the "preferred" approach is
`dplyr::group_by()` + `tidyr::nest()` for splitting,
`dplyr::mutate()` + `purrr::map()` for applying,
and `tidyr::unnest()` for combining.

Additionally, many in the community have shown implementations
of the ["many models"](http://r4ds.had.co.nz/many-models.html)
approach in `{tidyverse}`-style pipelines, often
also using the `{broom}` package. For example, see any one of Dr. Simon J's
many blog posts on machine learning, such as
[this one on k-fold cross validation](https://drsimonj.svbtle.com/k-fold-cross-validation-with-modelr-and-broom).

However, I haven't seen as much exploration of how to apply the split-apply-combine
technique to machine learning with the `{caret}` package, which is perhaps
the most popular "generic" `R` machine learning package (along with `{mlr}`).
One interesting write-up that I found on this subject is 
[this one by Rahul S.](https://rsangole.netlify.com/post/pur-r-ify-your-carets/).
Thus, I was inspired to experiment with my own `{tidyverse}`-like
pipelines using `{caret}`. (I actually used these techniques in my
homework solutions to the [edX](https://www.edx.org/) Georgia Tech 
[_Introduction to Analytics Modeling_ class](https://pe.gatech.edu/courses/introduction-analytics-modeling)
that I have been taking this summer.)

## Setup

For this walk-through, I'll be exploring the `PrimaIndiansDiabetes` data set
provided by the `{mlbench}` package. This data set
was originally collected by the 
[National Institute of Diabetes and Digestive and Kidney Diseases](https://www.niddk.nih.gov/) and
[published as one of the many datasets available in the UCI Repository](http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes).
It consists of 768 rows and 9 variables. It is useful for practicing
binary classification, where the 
 `diabetes` class variable (consisting of `pos` and `neg` values) is the response
which I aim to predict.

```{r data}
data("PimaIndiansDiabetes", package = "mlbench")
```

```{r data_show}
PimaIndiansDiabetes
```

```{r fmla_diabetes}
fmla_diabetes <- formula(diabetes ~ .)
```

Additionally, I'll be using the `{tidyverse}` suite of packages---most notably
`{dplyr}`, `{purrr}`, and `{tidyr}`---as well as the `{caret}` package for its machine learning API.


```{r packages}
library("tidyverse")
library("caret")
```

## Traditional `{caret}` Usage


First, I think it's instructive to show how one might typically use `{caret}`
to create individual models so that I can create a baseline with which to compare
a "many models" approach.

So, let's say that I want to fit a cross-validated CART 
([classification and regression tree](https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/)) model
with scaling and a grid of reasonable complexity parameter (`cp`) values.
(I don't show the output here because the code is shown purely for exemplary purposes.)

```{r fit_rpart, eval = FALSE}
fit_rpart <-
  train(
    form = fmla_diabetes,
    data = PimaIndiansDiabetes,
    method = "rpart",
    preProcess = "scale",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    minsplit = 5,
    tuneGrid = data.frame(cp = 10 ^ seq(-2, 1, by = 1))
  )
fit_rpart
```

It's reasonable to use `{caret}` directly in the manner shown above when simply
fitting one model.
But, let's say that now you want to compare the previous results
with a model fit with un-scaled predictors (perhaps for pedagogical purposes. Now you copy-paste the previous
statement, only modifying `preProcess` from `"scale"` to `NULL`.

```{r fit_rpart_unscaled, eval = FALSE}
fit_rpart_unscaled <-
  train(
    form = fmla_diabetes,
    data = PimaIndiansDiabetes,
    method = "rpart",
    preProcess = NULL,
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    minsplit = 5,
    tuneGrid = data.frame(cp = 10 ^ seq(-2, 1, by = 1))
  )
fit_rpart_unscaled
```

Although I might feel bad about copying-and-pasting so much code, I
end up with what I wanted.

But now I want to try a different method--a random forest. Now I will
need to change the value of the `method` argument (to `"rf"`) __and__ the value of 
`tuneGrid`---because there are different parameters to tune for a 
random forest---__and__
remove the `minsplit` argument---because it is not applicable for the
`caret::train()` method for `"rf"`, and, consequently, will cause an error.

```{r fit_rf, eval = FALSE}
fit_rf <-
  train(
    form = fmla_diabetes,
    data = PimaIndiansDiabetes,
    method = "rf",
    preProcess = "scale",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    tuneGrid = tuneGrid = data.frame(mtry = c(3, 5, 7))
  )
fit_rf
```

Then, what if I want to try yet another different method (with the same formula (`form`)
and `data`)? I would have to continue copy-pasting like this, which, of course,
can start to become tiresome.
The [DRY principle](https://tonyelhabr.rbind.io/post/dry-principle-make-a-package/)
is certainly relevant here---an approach using functions to automate the re-implementation
the "constants" among methods is superior.

Anyways, I think it is evident that a better approach than copy-pasting code
endlessly can be achieved.
With that said, next I'll demonstrate two approaches.
While I believe the second of these is superior because it is less verbose,
I think it's worth showing the first approach as well for instructive
purposes.

## split-apply-combine + `{caret}`, Approach #1

For this first approach,

1. First, I define
a "base" function that creates a list of arguments passed to `caret::train()`
that are common among each of the methods to evaluate.
2. Next, I define several "method-specific" functions
for `rpart"`, `"rf"`, and `"ranger"` (a faster implementation of the random forest
than that of the more well known `{randomForest}` package, which
is used by the `"rf"` method for `caret::train()`). These functions return lists of
parameters for `caret::train()` that are unique for each function---most notably `tuneGrid`.
(The `{caret}` [package's documentation](https://topepo.github.io/caret) should be consulted to identify exactly
which arguments must be defined.)
Additionally, note that there may be parameters that are passed
directly to the underlying function called by `caret::train()` (via the `...` argument).
such as `minsplit` for the `"rpart"`. (
3. Finally, I define a `sprintf`-style
function---with `method` as an argument---to call the method-specific functions (using `purrr::invoke()`).

Thus, the call stack for this approach looks like this. [^diagram]

[^diagram]:
This figure is created with the awesome `{DiagrammeR}` package.

```{r approach1_diagram}
DiagrammeR::grViz("
digraph {

  graph [overlap = true, fontsize = 10]

  node [shape = plaintext,
        fontname = Arial]

  a [label = '@@1']

  node [shape = box,
        fontname = Arial]

  b [label = '@@2']
  c [label = '@@3']
  d [label = '@@4']

  a->b
  b->c
  b->d
}

[1]: stringr::str_wrap('{caret} parameters', 60)
[2]: stringr::str_wrap('sprintf function', 60)
[3]: stringr::str_wrap('base {caret} function', 60)
[4]: stringr::str_wrap('method-specific {caret} function', 60)
")
```

```{r setup_caret_tree_funcs}
setup_caret_base_tree <-
  function() {
    list(
      form = fmla_diabetes,
      data = PimaIndiansDiabetes,
      trControl = trainControl(method = "cv", number = 5),
      metric = "Accuracy"
    )
  }

setup_caret_rpart <-
  function() {
    list(method = "rpart",
         minsplit = 5,
         tuneGrid = data.frame(cp = 10 ^ seq(-2, 1, by = 1)))
  }

setup_caret_rf <-
  function() {
   list(method = "rf", tuneGrid = data.frame(mtry = c(3, 5, 7)))
  }

setup_caret_ranger <-
  function() {
    list(method = "ranger",
         tuneGrid = 
           expand.grid(
             mtry = c(3, 5, 7),
             splitrule = c("gini"),
             min.node.size = 5,
             stringsAsFactors = FALSE
            )
    )
  }

fit_caret_tree_sprintf <-
  function(method = NULL, preproc = NULL) {
    invoke(
      train,
      c(
        invoke(setup_caret_base_tree),
        invoke(sprintf("setup_caret_%s", method)),
        preProcess = preproc
      )
    )
  }
```

(I apologize if the `_tree` suffix with the functions defined here seems verbose,
but I think this syntax servers as an informational "hint" that other
functions with non-tree-based methods could be written in a similar fashion.)

Next, I define the "grids" of method and pre-processing specifications to
pass to the functions. I define a relatively "minimal" set of different combinations
in order to emphasize the functionality that is implemented (rather than the choices
for methods and pre-processing).

Note the following:

+ The `_desc` column(s) are purely for informative purposes---they aren't used 
in the functions.
+ The `idx_method` column is defined for use as the key column in the join
that it does to combine these "grid" specifications and the functions
when the functions are called. (See `fits_diabetes_tree`.)
+ I want to try methods without any pre-processing,
meaning that `preProcess` should be set to `NULL` in `caret::train()`. However,
it is not possible (to my knowledge) to explicitly define a `NULL` in a data.frame,
so I use the surrogate `"none"`, which gets ignored (i.e. treated as `NULL`)
when passed as the value of `preProcess` to `caret::train()`.
+ The `method` and `preproc` columns are not actually used directly
for this approach, but for the next one.


```{r grid_preproc}
grid_preproc <-
  tribble(
    ~preprocess_desc, ~preproc,
    "Scaled", "scale",
    "Unscaled", "none"
  )
```

```{r grid_methods_tree}
grid_methods_tree <-
  tribble(
    ~method_desc, ~method,
    "{rpart} CART", "rpart",
    "{randomForest} Random Forest", "rf",
    "{ranger} Random Forest", "ranger"
  ) %>% 
  crossing(
    grid_preproc
  ) %>% 
  unite(method_desc, method_desc, preprocess_desc, sep = ", ") %>% 
  mutate(idx_method = row_number()) %>% 
  select(idx_method, everything())
grid_methods_tree
```

Finally, for the actual implementation, I call `fit_caret_tree_sprintf()`
for each combination of method and pre-processing transformation(s).
Importantly, the calls should be made in the same order as
that implied by `grid_methods_tree` so that the join on `idx_method`
is "valid" (in the sense that the model fit aligns with the description).

```{r fits_diabetes_tree_1, results = "hide", eval = FALSE}
set.seed(42)
fits_diabetes_tree_1 <-
  tribble(
    ~fit,
    fit_caret_tree_sprintf(method = "rpart", preproc = NULL),
    fit_caret_tree_sprintf(method = "rpart", preproc = "scale"),
    fit_caret_tree_sprintf(method = "rf", preproc = NULL),
    fit_caret_tree_sprintf(method = "rf", preproc = "scale"),
    fit_caret_tree_sprintf(method = "ranger", preproc = NULL),
    fit_caret_tree_sprintf(method = "ranger", preproc = "scale")
  ) %>% 
  mutate(idx_method = row_number()) %>% 
  left_join(grid_methods_tree) %>% 
  # Rearranging so that `fit` is the last column.
  select(-fit, everything(), fit)
fits_diabetes_tree_1
```

```{r fits_diabetes_tree_1_export, include = FALSE, eval = FALSE}
teproj::export_ext_rds(
  fits_diabetes_tree_1,
  export = config$export_data,
  dir = config$dir_data
)
```

```{r fits_diabetes_tree_1_import, include = FALSE, eval = FALSE}
fits_diabetes_tree_1 <-
  teproj::import_ext_rds(
    fits_diabetes_tree_1,
    dir = config$dir_data
  )
```

```{r fits_diabetes_tree_1_show, echo = FALSE}
fits_diabetes_tree_1
```

I get the results that I wanted, albeit with a bit more verbosity than
I might have liked. (Note the repeated
calls to the sprintf function.)

There are a couple of other things that I did not mention
before about the code that I think are worth saying.

+ If one does not wish to specify `tuneGrid`, then the implementation becomes simpler.
+ Other `purrr` functions such as `partial()` or `compose()` could possibly be used in some
manner to reduce some of the redundant code to an even greater extent.
+ If one only wants to implement pre-processing for certain methods, then
the method-specific functions and the sprintf function could be re-defined
such that `preproc` is passed as an argument to the method-specific functions and
not used in the sprintf function.

## split-apply-combine + `{caret}`, Approach #2

For this next approach (my preferred one), the fundamental difference
is with the use of the `grid_methods_tree` data.frame created before.
Because the split-apply-combine recipe for this approach
directly uses the `method` and `preproc` columns---recall
that these are not used with the previous approach---it is necessary
to use `grid_methods_tree` at the beginning of the modeling pipeline.

Because I use the `grid_methods_tree` columns directly,
I remove much of the verbosity seen in the prior approach.
Now I define a function that essentially serves the same purpose as the sprintf function
defined before---it binds the lists returned by a base `{caret}` function
and a method-specific `{caret}` function, as well as a value for the `preProcess` argument.

There is a bit of "hacky" part of the implementation here---I
use a `switch()` statement in order to substitute `NULL` for `"none"`.
As mentioned before, it does not seem possible to create a `NULL` value
in a data.frame, so `"none"` serves as a "stand-in". Notably, an `ifelse()`
call does not work because it cannot return a `NULL`.

```{r fit_caret_tree}
fit_caret_tree <-
  function(method = NULL, preproc = "scale") {
    invoke(
      train,
      c(
        invoke(setup_caret_base_tree),
        invoke(sprintf("setup_caret_%s", method)),
        preProcess = switch(preproc == "none", NULL, preproc)
      )
    )
  }
```


```{r fits_diabetes_tree, results = "hide", eval = FALSE}
set.seed(42)
fits_diabetes_tree_2 <-
  grid_methods_tree %>%
  group_by(idx_method, method_desc) %>% 
  nest() %>% 
  mutate(fit = purrr::map(data, ~fit_caret_tree(method = .x$method, preproc = .x$preproc))) %>% 
  ungroup()
fits_diabetes_tree_2
```

```{r fits_diabetes_tree_2_export, include = FALSE, eval = FALSE}
teproj::export_ext_rds(
  fits_diabetes_tree_2,
  export = config$export_data,
  dir = config$dir_data
)
```

```{r fits_diabetes_tree_2_import, include = FALSE, eval = FALSE}
fits_diabetes_tree_2 <-
  teproj::import_ext_rds(
    fits_diabetes_tree_2,
    dir = config$dir_data
  )
```

```{r fits_diabetes_tree_2_show, echo = FALSE}
fits_diabetes_tree_2
```

Cool! This approach complies with the modern split-apply-combine approach---`dplyr::group_by()` +
 `tidyr::nest()`,
`dplyr::mutate()` + `purrr::map()`
and `tidyr::unnest()`---and achieves the results which I sought.

## Quantifying Model Quality

So I've got the fitted models for "many models" in a single `tibble`.
How do I extract the results from this? `{purrr}`'s `pluck()` function
is helpful here, along with the `dplyr::mutate()`, `purrr::map()`, and `tidyr::unnest()`
functions used before.

```{r unnest_caret_results}
unnest_caret_results <-
  function(fit = NULL, na.rm = TRUE) {
    fit %>%
      mutate(results = map(fit, ~pluck(.x, "results"))) %>% 
      unnest(results, .drop = TRUE)
  }
```

We get the same exact results with both approaches demonstrated above, so I'll only
show the results for one here.

```{r summ_diabetes_tree}
summ_diabetes_tree <-
  fits_diabetes_tree_2 %>%
  unnest_caret_results() %>% 
  select(-matches("SD")) %>% 
  arrange(desc(Accuracy))
summ_diabetes_tree
```

And, with the results fashioned like this, we can easily perform other typical
`{dplyr}` actions to gain insight from the results quickly.

```{r summ_diabetes_bymethod}
summ_diabetes_bymethod <-
  summ_diabetes_tree %>% 
  group_by(idx_method, method_desc) %>% 
  summarise_at(vars(Accuracy), funs(min, max, mean, n = n())) %>% 
  ungroup() %>% 
  arrange(desc(max))
summ_diabetes_bymethod
```

And let's not forget about visualizing the results.

```{r viz_summ_diabetes_bymethod, include = FALSE, fig.show = "hide"}
library("teplot")
viz_summ_diabetes_bymethod <-
  summ_diabetes_tree %>% 
  ggplot(aes(x = method_desc, y = Accuracy, fill = method_desc)) +
  geom_point(size = 5, shape = 21) +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 12)) +
  # teplot::scale_fill_set1() +
  scale_fill_hue(l = 55) +
  # guides(color = guide_legend(override.aes = list(size = 5))) +
  teplot::theme_te() +
  theme(legend.position = "none") +
  labs(title = str_wrap("Cross Validation Accuracy of Split-Apply-Combine {caret} Models", 100),
       caption = "By Tony ElHabr",
       x = NULL,
       y = NULL)
viz_summ_diabetes_bymethod
```

```{r viz_summ_diabetes_bymethod_show, echo = FALSE, fig.height = 6, fig.width = 7}
viz_summ_diabetes_bymethod
```

```{r viz_summ_diabetes_bymethod_export, include = FALSE, eval = FALSE}
teproj::export_ext_png(
  viz_summ_diabetes_bymethod,
  export = config$export_viz,
  dir = config$dir_viz,
  units = "in",
  height = 6,
  width = 7
)
```

## Conclusion

That's it for this tutorial. I hope that this is useful for others. (If nothing else,
it's useful for me to review.) While the `{caret}` package does allow users
[to create custom `train()` functions](https://topepo.github.io/caret/using-your-own-model-in-train.html), 
I believe that this functionality
is typically beyond what a user needs when experimenting with different approaches
(and can be quite complex). I believe that the approach (primarily the second one) that
I've shown in this post offers a great amount of flexibility that complements
the many other aspects of `{caret}`'s user-friendly, dynamic API.

In an actual analysis, the approach(es) that I've shown
can be extremely useful in experimenting
with many different methods, parameters, and data pre-processing in order
to identify a set that is most appropriate for the context.


```{r save, include = FALSE}
# NOTE TO SELF: Do not save with blog post or if calling rmarkdown::render().
# save.image(file = path_save)
```

