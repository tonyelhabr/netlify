---
title: Dealing with Interval Data and the nycflights13 package using R
date: "2018-02-17"
slug: interval-data-nycflights13
categories: []
tags:
  - r
  - dplyr
  - nycflights13
banner: "img/interval-data-nycflights13/viz_nycflights13-banner.png"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE
)
```

In my job, I often work with data sampled at regular intervals.
Samples may range from 5-minute intervals to daily intervals, depending on the specific task.
While working with this kind of data is straightforward when its in a database (and I can use SQL),
I have been in a couple of situations where the data is spread across .csv files.
In these cases, I lean on `R` to scrape and compile the data.
Although some other languages might be superior for such a task, I often
need to produce some kind of visualization or report at the end, so
choosing to handle all of the data with `R` is a no-brainer for me--I can
easily transition to using `ggplot2` and `rmarkdown` to generate some pretty output.

## Minute-Level Data

When working with data sampled at 5-minute intervals
(resulting in 288 intervals in a day), I've found that I've used a common
"idiom" to generate a time-based "dictionary".
For example, here's how I might create such a date dictionary for the year 2013. [^fn_technique]

[^fn_technique]:
The technique that I show here 
would have to be adjusted slightly if working with more than one year
at a time, but it wouldn't be difficult to do so. 
I tend to only use this design pattern for one year at a time anyways.


```{r dates_dict}
library("dplyr")
library("lubridate")

date_1 <- lubridate::ymd("2013-01-01")
date_2 <- lubridate::ymd("2013-12-31")

yyyymmdd_seq <- seq.Date(date_1, date_2, by = "day")
yyyymmdd_grid <-
  data_frame(
    yyyymmdd = lubridate::ymd(yyyymmdd_seq),
    yyyy = lubridate::year(yyyymmdd_seq),
    mm = lubridate::month(yyyymmdd_seq),
    dd = lubridate::day(yyyymmdd_seq)
  )

hhmmss <-
  expand.grid(
    hh = seq(1L, 24L, 1L), 
    min = seq(5L, 60L, by = 5L), 
    sec = 0L) %>% 
  as_tibble()

dates_dict <-
  yyyymmdd_grid %>%
  right_join(hhmmss %>% mutate(yyyy = lubridate::year(date_1), by = "year")) %>% 
  arrange(yyyy, mm, dd, hh, min)
dates_dict
```

And, just to prove that there are 288 5-minute intervals for each day in the year,
I can use `dplyr::count()` twice in succession. [^fn_count_twice]

```{r dates_dict_nn}
dates_dict %>% count(yyyy, mm, dd) %>% count()
```

[^fn_count_twice]:
By the way, how cool is following one `dplyr::count()` call with another? I only
found out about that nice little trick recently.

I then extract data (from individual files) using the time-based dictionary
as a "helper" for custom functions for creating file paths and processing the data
after importing it.

After the dirty work of is done, I can transition to the fun part--exploring
and interpreting the data.
This process often turns out to be a cycle of visualization, data transformation,
and modeling.

![](/img/visualizing-nba-team-schedule/data-science.png)

### An Example (With the `nycflights13` Package)

To provide an example, I'll use the `flights` data set from the 
[`nycflight13`](https://cran.r-project.org/web/packages/nycflights13/index.html) package. [^fn_hadley]
This package includes information regarding all flights leaving from New York City
airports in 2013, as well as information regarding weather, airlines, airports, and planes.

[^fn_hadley]:
Thanks to [Hadley Wickham](http://hadley.nz/)  for compiling this data package, 
as well as for developing the `lubridate` and `ggplot2` packages.)

Let's say that I that I'm interested in the average flight departure delay time
at the JFK airport. I might hypothesize that there is a relationship between
departure delay time with different time periods, 
such as hour in the day and days in the week.

First, I'll perform the necessary transformation to the `flights` data to investigate my hypothesis.
Specifically, I need to create columns for hour and weekday. (For hour (`hh`),
I simply use the scheduled departure time (`sched_dep_time`).)

```{r flights_processed}
library("nycflights13")

flights_jfk <-
  nycflights13::flights %>% 
  filter(origin == "JFK") %>% 
  mutate(hh = round(sched_dep_time / 100, 0) - 1) %>% 
  mutate(yyyymmdd = lubridate::ymd(sprintf("%04.0f-%02.0f-%02.0f", year, month, day))) %>% 
  mutate(wd = lubridate::wday(yyyymmdd, label = TRUE))
```

Next, I might create a heat map plotting hours against weekdays.

```{r viz_nycflight13, echo = FALSE, fig.width = 8, fig.height = 6}
library("ggplot2")

viz_nycflights13 <-
  flights_jfk %>% 
  group_by(wd, hh) %>% 
  summarize_at(vars(dep_delay), funs(mean(., na.rm = TRUE))) %>% 
  ungroup() %>% 
  mutate(hh = hh - 1) %>% 
  ggplot(aes(x = hh, y = wd, fill = dep_delay)) +
  geom_tile() +
  coord_equal() +
  viridis::scale_fill_viridis(option = "C", name = "Minutes") +
  # guides(fill = guide_legend(title = "Minutes")) +
  theme_minimal() +
  theme(legend.position = "right", panel.grid.major = element_blank()) +
  labs(
    x = NULL,
    y = NULL,
    title = "Average Departure Delays at JFK Airport in 2013",
    subtitle = "By Hour and Weekday",
    caption = 
      paste0("There appears to be more delays near the end of the day.\n",
             "It's not clear whether there is a relationship with certain days of the week.")
  )
viz_nycflights13
```

```{r save_viz_nycflights13, include = FALSE, eval = FALSE}
ggsave(viz_nycflights13, file = "static/img/interval-data-nycflights13/viz_nycflights13-banner.png", units = "in", width = 8, height = 4)
```

To investigate the patterns more "scientifically", I might perform a one-way
Analysis of Variance (ANOVA) on different time variables.
I would make sure to test time periods other than just
weekday (`wd`) and hour (`hh`), such as `month` and `day`.

```{r anova_oneway}
summary(aov(dep_delay ~ month, data = flights_jfk))
summary(aov(dep_delay ~ day, data = flights_jfk))
summary(aov(dep_delay ~ wd, data = flights_jfk))
summary(aov(dep_delay ~ hh, data = flights_jfk))

```

The "statistically significant" p values for the `wd` and `hh` variables
provides incentive to investigate them more closely.
I might try an ANOVA F-statistic test comparing linear regression models
using each variable as a lone predictor with a linear model where both are used
as predictors.

```{r anova_hypothesis}

lm_wd <- lm(dep_delay ~ wd, data = flights_jfk)
lm_hh <- lm(dep_delay ~ hh, data = flights_jfk)
lm_both <- lm(dep_delay ~ wd + hh, data = flights_jfk)
anova(lm_both, lm_wd, test = "F")
anova(lm_both, lm_hh, test = "F")
```

Of course, this process would go on. For instance, I would certainly need to investigate
if there is a relationship between departure delay and specific airlines.

## Final Thoughts

Working with interval data was initially a challenge for me, but after working with it
more and more often, I find that it's not so bad after all. It gets more interesting
when there is missing data or data samples at irregular intervals, but that's a story
for another day.


