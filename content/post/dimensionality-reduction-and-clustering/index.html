---
title: 'Tired: PCA + kmeans, Wired: UMAP + GMM'
date: '2021-06-30'
categories:
  - r
tags:
  - r
  - soccer
image:
  caption: ''
  focal_point: ''
  preview_only: true
header:
  caption: ''
  image: 'featured.png'
output:
  html_document:
    keep_md: yes
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Combining <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">principal component analysis (PCA)</a> and <a href="https://en.wikipedia.org/wiki/K-means_clustering">kmeans clustering</a> seems to be a pretty popular 1-2 punch in data science. While there is some debate about whether combining <a href="https://en.wikipedia.org/wiki/Dimensionality_reduction">dimensionality reduction</a> and <a href="https://en.wikipedia.org/wiki/Clustering">clustering</a> is something we should ever do<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>, I’m not here to debate that. I’m here to illustrate the potential advantages of upgrading your PCA + kmeans workflow to <a href="https://umap-learn.readthedocs.io/en/latest/">Uniform Manifold Approximation and Projection (UMAP)</a> + <a href="https://en.wikipedia.org/wiki/Mixture_model">Gaussian Mixture Model (GMM)</a>, as noted in <a href="https://twitter.com/TonyElHabr/status/1400149998629703681">my reply here</a>.</p>
<p><img src="tweet.png" /></p>
<p>For this demonstration, I’ll be using <a href="https://docs.google.com/spreadsheets/d/1lQgIDcxsHT1m_IayMldmiHVOt4ICbX-ys8Mh9rggPHM/edit?usp=sharing">this data set</a> pointed out <a href="https://twitter.com/ronanmann/status/1408504415690969089?s=21">here</a>, including over 100 stats for players from <a href="https://fbref.com/en/comps/Big5/Big-5-European-Leagues-Stats">soccer’s “Big 5” leagues</a>.</p>
<pre class="r"><code>library(tidyverse)
df &lt;- 
  &#39;FBRef 2020-21 T5 League Data.xlsx&#39; %&gt;% 
  readxl::read_excel() %&gt;% 
  janitor::clean_names() %&gt;% 
  mutate(across(where(is.double), ~replace_na(.x, 0)))

# Let&#39;s only use players with a 10 matches&#39; worth of minutes.
df_filt &lt;- df %&gt;% filter(min &gt; (10 * 90))</code></pre>
<pre class="r"><code>df_filt %&gt;% dim()</code></pre>
<pre class="r"><code># [1] 1626  128</code></pre>
<p>Trying to infer something from the correlation matrix doesn’t get you very far, so one can see why dimensionality reduction will be useful.</p>
<p><img src="viz_cors.png" /></p>
<p>Also, we don’t really have “labels” here (more on this later), so clustering can be useful for learning something from our data.</p>
</div>
<div id="unsupervised-evaluation" class="section level2">
<h2>Unsupervised Evaluation</h2>
<p>We’ll be feeding in the results from the dimensionality reduction—either PCA or UMAP—to a clustering method—either kmeans or GMM. So, since clustering comes last, all we need to do is figure out how to judge the clustering; this will tell us something about how “good” the combination of dimensionality reduction and clustering is overall.</p>
<p>I’ll save you from google-ing and just tell you that <a href="https://en.wikipedia.org/wiki/Total_sum_of_squares">within-cluster sum of squares (WSS)</a> is typically used for kmeans, and <a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">Bayesian Information Criteria (BIC)</a> is the go-to metric for GMM. WSS and BIC are not on the same scale, so we can’t directly compare kmeans and GMM at this point. Nonetheless, we can experiment with different numbers of components—the one major <a href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)">“hyperparameter”</a> for dimensionality reduction—prior to the clustering to identify if more or less components is “better”, given the clustering method. Oh, and why not also vary the number of clusters—the one notable hyperparameter for clustering—while we’re at it?</p>
<p><img src="viz_kmeans_wss.png" /></p>
<p>For kmeans, we see that WSS decreases with increasing number of clusters, which is typically what we see in [“elbow” plots](<a href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)" class="uri">https://en.wikipedia.org/wiki/Elbow_method_(clustering)</a> like this. Additionally, we see that WSS decreases with increasing number of components. This makes sense—additional components means more data is accounted for.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> There is definitely a point of “diminishing returns”, somewhere around 3 clusters, after which WSS barely improves.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> Overall, we observe that the kmeans models using UMAP pre-processing do better, compared to those using PCA.</p>
<p>Moving on to GMM, we observe that BIC generally increases with the number of clusters as well. (Note that, due to the way <a href="https://stats.stackexchange.com/questions/237220/mclust-model-selection">the <code>{mclust}</code> package defines it’s objective function, higher BIC is “better”</a>.)</p>
<p><img src="viz_gmm_bic.png" /></p>
<p>Regarding number of components, we see that the GMM models using more UMAP components do better, as we should have expected. On the other hand, we observe that GMM models using less PCA components do better than those with more components! This is a bit of an odd finding that I don’t have a great explanation for. (Someone please math-splain to me.) Nonetheless, we see that UMAP does better than PCA overall, as we observed with kmeans.</p>
<p>For those interested in the code, I <code>map</code>-ed a function across a grid of parameters to generate the data for these plots.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<pre class="r"><code>do_dimr_clust &lt;-
  function(n, k,
           f_dimr = c(&#39;pca&#39;, &#39;umap&#39;),
           f_clust = c(&#39;kmeans&#39;, &#39;gmm&#39;),
           ...) {
    f_dimr &lt;- match.arg(f_dimr)
    f_clust &lt;- match.arg(f_clust)
    f_step &lt;- ifelse(f_dimr == &#39;pca&#39;, recipes::step_pca, embed::step_umap)
    f_fit &lt;- ifelse(f_clust == &#39;gmm&#39;, stats::kmeans, mclust::Mclust)
    
    data &lt;-
      recipes::recipe(formula( ~ .), data = df_filt) %&gt;%
      recipes::step_normalize(recipes::all_numeric_predictors()) %&gt;%
      f_step(recipes::all_numeric_predictors(), num_comp = n) %&gt;% 
      recipes::prep() %&gt;% 
      recipes::juice() %&gt;% 
      select(where(is.numeric))
    fit &lt;- f_fit(data, ...)
    broom::glance(fit)
  }

metrics &lt;-
  crossing(
    n = seq.int(2, 8),
    k = seq.int(2, 8),
    f_dimr = c(&#39;pca&#39;, &#39;umap&#39;),
    f_clust = c(&#39;kmeans&#39;, &#39;mclust&#39;)
  ) %&gt;%
  mutate(metrics = pmap(
    list(n, k, f, g),
    ~ do_dimr_clust(
      n = ..1,
      k = ..2,
      f = ..3,
      g = ..4
    )
  ))
metrics</code></pre>
<pre class="r"><code># # A tibble: 196 x 5
#        n     k f     g      metrics         
#    &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;list&gt;          
#  1     2     2 pca   kmeans &lt;tibble [1 x 4]&gt;
#  2     2     2 pca   gmm    &lt;tibble [1 x 7]&gt;
#  3     2     2 umap  kmeans &lt;tibble [1 x 4]&gt;
#  4     2     2 umap  gmm    &lt;tibble [1 x 7]&gt;
#  5     2     3 pca   kmeans &lt;tibble [1 x 4]&gt;
#  6     2     3 pca   gmm    &lt;tibble [1 x 7]&gt;
#  7     2     3 umap  kmeans &lt;tibble [1 x 4]&gt;
#  8     2     3 umap  gmm    &lt;tibble [1 x 7]&gt;
#  9     2     4 pca   kmeans &lt;tibble [1 x 4]&gt;
# 10     2     4 pca   gmm    &lt;tibble [1 x 7]&gt;
# # ... with 186 more rows</code></pre>
</div>
<div id="supervised-evaluation" class="section level2">
<h2>“Supervised” Evaluation</h2>
<p>We actually do have something that we can use to help us identify clusters—player position (<code>pos</code>). Let’s treat these position groups as pseudo-labels with which we can gauge the effectiveness of the clustering.</p>
<pre class="r"><code>df_filt &lt;-
  df_filt %&gt;% 
  mutate(
    across(
      pos,
      ~case_when(
        .x %in% c(&#39;DF,MF&#39;, &#39;MF,DF&#39;) ~ &#39;DM&#39;,
        .x %in% c(&#39;DF,FW&#39;, &#39;FW,DF&#39;) ~ &#39;M&#39;,
        .x %in% c(&#39;MF,FW&#39;, &#39;FW,MF&#39;) ~ &#39;AM&#39;,
        .x == &#39;DF&#39; ~ &#39;D&#39;,
        .x == &#39;MF&#39; ~ &#39;M&#39;,
        .x == &#39;FW&#39; ~ &#39;F&#39;,
        .x == &#39;GK&#39; ~ &#39;G&#39;,
        .x == &#39;GK,MF&#39; ~ &#39;G&#39;,
        TRUE ~ .x
      )
    )
  )
df_filt %&gt;% count(pos, sort = TRUE)</code></pre>
<pre class="r"><code># # A tibble: 6 x 2
#   pos       n
#   &lt;chr&gt; &lt;int&gt;
# 1 D       595
# 2 M       364
# 3 AM      273
# 4 F       196
# 5 G       113
# 6 DM       85</code></pre>
<p>Typically we don’t have labels for clustering tasks; if we do, we’re usually doing some kind of supervised multi-label classification. But our labels aren’t “true” labels in this case, both because:</p>
<ol style="list-style-type: decimal">
<li>a player’s nominal position often doesn’t completely describe their style of play, and</li>
<li>the grouping I did to reduce the number of positions from 11 to 6 was perhaps not optimal.</li>
</ol>
<p>So now let’s do the same as before—evaluate different combinations of PCA and UMAP with kmeans and GMM. But now we can use some supervised evaluation metrics: (1) <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and (2) mean <a href="https://en.wikipedia.org/wiki/Cross_entropy">log loss</a>. While the former is based on the “hard” predictions, the latter is based on probabilities for each class. kmeans returns just hard cluster assignments, so computing accuracy is straightforward; since it doesn’t return probabilities, we’ll treat the hard assignments as having a probability of 1 to compute log loss.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p>We can compare the two clustering methods more directly now using these two metrics. Since we know that there are 6 position groups, we’ll keep the number of clusters constant at 6. (Note that number of <strong>clusters</strong> was shown on the x-axis before; but since we have fixed number of components at 6, now we show the number of <strong>components</strong> on the x-axis.)</p>
<p>Looking at accuracy first, we see that the best combo depends on our choice for number of components. Overall, we might say that the UMAP combos are better.</p>
<p><img src="viz_acc.png" /></p>
<p>Next, looking at average log loss, we see that the GMM clustering methods seem to do better overall (although this may be due to the fact that log loss is not typically used for supervised kmeans). The PCA + GMM does the best across all number of components, with the exception of 7. Note that we get a mean log loss around 28 when we predict the majority class (defender) with a probability of 1 for all observations. (This is a good “baseline” to contextualize our numbers.)</p>
<p><img src="viz_ll.png" /></p>
<p>UMAP shines relative to PCA according to accuracy, and GMM beats out kmeans in terms of log loss. Despite these conclusions, we still don’t have clear evidence that UMAP + GMM is the best 1-2 combo; nonetheless, we can at least feel good about its general strength.</p>
<div id="aside-re-coding-clusters" class="section level3">
<h3>Aside: Re-coding Clusters</h3>
<p>I won’t bother to show all the code to generate the above plots since it’s mostly just <code>broom::augmment()</code> and <code>{ggplot2}</code>. But, if you have ever worked with supervised stuff like this (if we can call it that), you’ll know that figuring out which of your clusters correspond to your known groups can be difficult. In this case, I started from a variable holding the predicted <code>.class</code> and the true class (<code>pos</code>).</p>
<pre class="r"><code>assignments</code></pre>
<pre class="r"><code># # A tibble: 1,626 x 2
#    .class pos  
#     &lt;int&gt; &lt;chr&gt;
#  1      1 D    
#  2      2 D    
#  3      3 M    
#  4      3 M    
#  5      4 AM   
#  6      2 D    
#  7      2 D    
#  8      4 F    
#  9      2 D    
# 10      1 D    
# # ... with 1,616 more rows</code></pre>
<p>I generated a correlation matrix for these two columns, ready to pass into a matching procedure.</p>
<pre class="r"><code>cors &lt;-
  assignments %&gt;% 
  fastDummies::dummy_cols(c(&#39;.class&#39;, &#39;pos&#39;), remove_selected_columns = TRUE) %&gt;% 
  corrr::correlate(method = &#39;spearman&#39;, quiet = TRUE) %&gt;% 
  filter(term %&gt;% str_detect(&#39;pos&#39;)) %&gt;% 
  select(term, matches(&#39;^[.]class&#39;))
cors</code></pre>
<pre class="r"><code># # A tibble: 6 x 7
#   term   .class_1 .class_2 .class_3 .class_4 .class_5 .class_6
#   &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
# 1 pos_AM  -0.208   -0.241   -0.178    0.0251   0.625   -0.123 
# 2 pos_D    0.499    0.615   -0.335   -0.264   -0.428   -0.208 
# 3 pos_DM   0.0797   0.0330   0.0548  -0.0829  -0.0519  -0.0642
# 4 pos_F   -0.171   -0.199   -0.168    0.724    0.0232  -0.101 
# 5 pos_G   -0.127   -0.147   -0.124   -0.0964  -0.157    1     
# 6 pos_M   -0.222   -0.267    0.724   -0.180    0.0395  -0.147</code></pre>
<p>Then I used <code>clue::solve_LSAP()</code> to do the <a href="https://en.wikipedia.org/wiki/Hungarian_algorithm">bipartite matching magic</a>. The rest is just pre- and post-processing.</p>
<pre class="r"><code>k &lt;- 6 # number of clusters
cols_idx &lt;- 2:(k+1)
cors_mat &lt;- as.matrix(cors[,cols_idx]) + 1 # all values have to be positive
rownames(cors_mat) &lt;- cors$term
cols &lt;- names(cors)[cols_idx]
colnames(cors_mat) &lt;- cols
cols_idx_min &lt;- clue::solve_LSAP(cors_mat, maximum = TRUE)
cols_min &lt;- cols[cols_idx_min]
pairs &lt;-
  tibble::tibble(
    .class = cols_min %&gt;% str_remove(&#39;^[.]class_&#39;) %&gt;% as.integer(),
    pos = cors$term %&gt;% str_remove(&#39;pos_&#39;)
  )
pairs</code></pre>
<pre class="r"><code># # A tibble: 6 x 2
#   .class pos  
#    &lt;int&gt; &lt;chr&gt;
# 1      5 AM   
# 2      2 D    
# 3      1 DM   
# 4      4 F    
# 5      6 G    
# 6      3 M </code></pre>
<p>This <code>pairs</code> variable can be used to re-code the <code>.class</code> column in our <code>assignments</code> from before.</p>
</div>
</div>
<div id="case-study-pca-vs.-umap" class="section level2">
<h2>Case Study: PCA vs. UMAP</h2>
<p>Let’s step back from the clustering techniques and focus on dimensionality reduction for a moment. One of the ways that dimensionality reduction can be leveraged in sports like soccer is for player similarity metrics.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> Let’s take a look at how this can be done, comparing the PCA and UMAP results while we’re at it.</p>
<p>Direct comparison of the similarity “scores” we’ll compute—based on Euclidean distance between a chosen player’s components and other players’ components—is not wise given the different ranges of our PCA and UMAP components, so we’ll rely on rankings based on these scores.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> Additionally, <a href="https://fbref.com/">fbref</a> provides a “baseline” that we can use to judge our similarity rankings.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<p>We’ll start with <a href="https://fbref.com/en/players/dbf053da/Jadon-Sancho">Jadon Sancho</a>, a highly discussed player at the moment (as a potential transfer).</p>
<p><img src="jadon-sancho-fbref.PNG" /></p>
<p>We first need to set up our data into the following format. (This is for 2-component, 6-cluster UMAP + GMM.)</p>
<pre class="r"><code>sims_int</code></pre>
<pre class="r"><code># # A tibble: 1,664 x 6
#    player_1     player_2           comp_1 comp_2 value_1 value_2
#    &lt;chr&gt;        &lt;chr&gt;               &lt;int&gt;  &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;
#  1 Jadon Sancho Aaron Leya Iseka        1      1  -4.18  -5.14  
#  2 Jadon Sancho Aaron Leya Iseka        2      2  -0.678  2.49  
#  3 Jadon Sancho Aaron Ramsey            1      1  -4.18  -3.25  
#  4 Jadon Sancho Aaron Ramsey            2      2  -0.678 -0.738 
#  5 Jadon Sancho Abdoul Kader Bamba      1      1  -4.18  -3.40  
#  6 Jadon Sancho Abdoul Kader Bamba      2      2  -0.678  0.0929
#  7 Jadon Sancho Abdoulaye Doucouré      1      1  -4.18  -1.36  
#  8 Jadon Sancho Abdoulaye Doucouré      2      2  -0.678 -2.66  
#  9 Jadon Sancho Abdoulaye Touré         1      1  -4.18  -1.36  
# 10 Jadon Sancho Abdoulaye Touré         2      2  -0.678 -2.89  
# # ... with 1,654 more rows</code></pre>
<p>Then the Euclidean distance calculation is fairly straightforward.</p>
<pre class="r"><code>sims &lt;-
  sims_init %&gt;% 
  group_by(player_1, player_2) %&gt;% 
  summarize(
    d = sqrt(sum((value_1 - value_2)^2))
  ) %&gt;% 
  ungroup() %&gt;% 
  mutate(score = 1 - ((d - 0) / (max(d) - 0))) %&gt;% 
  mutate(rnk = row_number(desc(score))) %&gt;% 
  arrange(rnk) %&gt;% 
  select(player = player_2, d, score, rnk)
sims</code></pre>
<pre class="r"><code># # A tibble: 830 x 4
#    player                  d score   rnk
#    &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
#  1 Alexis Sánchez     0.0581 0.994     1
#  2 Riyad Mahrez       0.120  0.988     2
#  3 Serge Gnabry       0.132  0.986     3
#  4 Jack Grealish      0.137  0.986     4
#  5 Pablo Sarabia      0.171  0.983     5
#  6 Thomas Müller      0.214  0.978     6
#  7 Leroy Sané         0.223  0.977     7
#  8 Callum Hudson-Odoi 0.226  0.977     8
#  9 Jesse Lingard      0.260  0.973     9
# 10 Ousmane Dembélé    0.263  0.973    10
# # ... with 820 more rows</code></pre>
<p>Doing the same for PCA and combining all results, we get the following set of rankings.</p>
<p><img src="gt_similarity_jadon_sancho.png" /></p>
<p>We see that the UMAP rankings are “closer” overall to the fbref rankings. Of course, there are some caveats:</p>
<ol style="list-style-type: decimal">
<li>This is just one player.</li>
<li>This is with a specific number of components and clusters.</li>
<li>We are comparing to similarity rankings based on a separate methodology.</li>
</ol>
<p>Our observation here (that UMAP &gt; PCA) shouldn’t be taken out of context to conclude that UMAP &gt; PCA in all contexts. Nonetheless, I think this is an interesting use case for dimensionality reduction, where one can justify PCA, UMAP, or any other similar technique, depending on how intuitive the results are.</p>
</div>
<div id="case-study-umap-gmm" class="section level2">
<h2>Case Study: UMAP + GMM</h2>
<p>Finally, let’s bring clustering back into the conversation. We’re going to focus on how the heralded UMAP + GMM combo can be visualized to provide insight that supports (or debunks) our prior understanding.</p>
<p>With a 2-component UMAP + 6-cluster GMM, we can see how the 6 position groups can be identified in a 2-D space.</p>
<p><img src="viz_uncertainty_umap_full.png" /></p>
<p>For those curious, using PCA instead of UMAP also leads to an identifiable set of clusters. However, uncertainties are generally higher across the board (larger point sizes, more overlap between covariance ellipsoids).</p>
<p><img src="viz_uncertainty_pca_full.png" /></p>
<p>If we exclude keepers (G) and defenders (D) to focus on the other 4 positions with our UMAP + GMM approach, we can better see how some individual points —at the edges or outside of covariance ellipsoids—are classified with a higher degree of uncertainty.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a></p>
<p><img src="viz_uncertainty_umap_filt.png" /></p>
<p>Now, highlighting incorrect classifications, we can see how the defensive midfielder (DM) position group (upper left) seems to be a blind spot in our approach.</p>
<p><img src="viz_misclassified_umap.png" /></p>
<p>A more traditional confusion matrix<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> also illustrates the inaccuracy with classifying DMs. (Note the lack of dark grey fill in the DM column.)</p>
<p><img src="viz_cm_umap.png" /></p>
<p>DMs are often classified as defenders instead. I think this poor result has is more so due to my lazy grouping of players with <code>"MF,DF'</code> or <code>"DF,MF"</code> positions in the original data set than a fault in our approach.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>So, should our overall conclusion be that we should never use PCA or kmeans? No, not necessarily. They can both be much faster to compute than UMAP and GMMs respectively, which can be a huge positive if computation is a concern. PCA is linear while UMAP is not, so you may want to choose PCA to make it easier to explain to your friends. Regarding clustering, kmeans is technically a specific form of a GMM, so if you want to sound cool to your friends and tell them that you use GMMs, you can do that.</p>
<p>Anyways, I hope I’ve shown why you should try out UMAP and GMM the next time you think about using PCA and kmeans.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>In some contexts you may want to do feature selection and/or manual grouping of data.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>While this whole thing is more about comparing techniques, I should make a note about WSS. We don’t want to increase the number of components for the sake of minimizing WSS. We lose some degree of interpretation with increasing components. Additionally, we could be <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> the model by increasing the number of components. Although we don’t have the intention of classifying new observations in this demo, it’s still good to keep overfitting in mind.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>This demo isn’t really intended to be a study in how to choose the best number of clusters, but I figured I’d point this out.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>I’d suggest <a href="https://juliasilge.com/blog/kmeans-employment/">this blog post from Julia Silge</a> for a better explanation of clustering with R and <a href="https://www.tidymodels.org/"><code>{tidymodels}</code></a>.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Perhaps this is against best practice, but we’ll do it here for the sake of comparison.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>Normalization perhaps doesn’t help much here given the clustered nature of the reduced data.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>Normalization perhaps doesn’t help much here given the clustered nature of the reduced data.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>fbref uses a different methodology, so perhaps it’s unwise to compare to them.<a href="#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>Sure, one can argue that a player like Diogo Jota should have been classified as an attacking midfielder (AM) to begin with, in which case he might not have been misclassified here.<a href="#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>By the way, the <code>autoplot()</code> function for <code>yardstick::conf_mat()</code> results is awesome if you haven’t ever used it.<a href="#fnref10" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
