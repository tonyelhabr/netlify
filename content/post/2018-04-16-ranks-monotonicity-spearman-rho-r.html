---
title: Investigating Ranks, Monotonicity, and Spearman's Rho with R
slug: ranks-monotonicity-spearman-rho-r
date: "2018-04-16"
categories: []
tags:
 - r
 - rank
 - monotonicity
 - spearman
 - dplyr
 - ggplot2
 - broom
banner: "img/ranks-monotonicity-spearman-rho-r/banner.png"
---



<div id="the-problem" class="section level2">
<h2>The Problem</h2>
<p>I have a bunch of data that can be categorized into many small <strong>groups</strong>. Each small group has a set of <strong>values</strong> for an ordered set of <strong>intervals</strong>. Having observed that the values for most groups seem to increase with the order of the interval, I hypothesize that their is a statistically-significant, monotonically increasing trend.</p>
<div id="an-analogy" class="section level3">
<h3>An Analogy</h3>
<p>To make this abstract problem more relatable, imagine the following scenario.</p>
<p>There are many companies (<strong>groups</strong>) selling products in an industry that is on the verge of widespread growth. Each company sets a projection (not a goal) for end-of-year sales (<strong>value</strong>). and adjust this projection once per quarter (i.e. four times a year) (<strong>interval</strong>) after analyzing their to-date sales. Having observed that most companies that follow this scheme tend to increase their goals for next-quarter sales (based on an initial, beginning-of-the-year projection that is too low), the market analyst (me) wonders if there is a non-trivial, positive trend across quarters in the year. <a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>:</p>
<p>To summarize how the “variables” in this analogy relate to my abstract description of the situation,</p>
<ul>
<li>groups = companies</li>
<li>value = sales</li>
<li>interval = quarter-year</li>
</ul>
<p>(Actually, there is one more variable in this context, given that I am interested in relative value—the <strong>rank</strong> of the value, relative to the other values.)</p>
<p>And, to make the example concrete, here is what the data (for four companies) might look like.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
company
</th>
<th style="text-align:left;">
q1
</th>
<th style="text-align:left;">
q2
</th>
<th style="text-align:left;">
q3
</th>
<th style="text-align:left;">
q4
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
A
</td>
<td style="text-align:left;">
3.6
</td>
<td style="text-align:left;">
8.5
</td>
<td style="text-align:left;">
9.2
</td>
<td style="text-align:left;">
9.4
</td>
</tr>
<tr>
<td style="text-align:left;">
B
</td>
<td style="text-align:left;">
-0.73
</td>
<td style="text-align:left;">
0.04
</td>
<td style="text-align:left;">
0.28
</td>
<td style="text-align:left;">
0.47
</td>
</tr>
<tr>
<td style="text-align:left;">
C
</td>
<td style="text-align:left;">
6604
</td>
<td style="text-align:left;">
4631
</td>
<td style="text-align:left;">
6604
</td>
<td style="text-align:left;">
7217
</td>
</tr>
<tr>
<td style="text-align:left;">
D
</td>
<td style="text-align:left;">
-49
</td>
<td style="text-align:left;">
-9
</td>
<td style="text-align:left;">
400
</td>
<td style="text-align:left;">
87
</td>
</tr>
</tbody>
</table>
<p><img src="/post/2018-04-16-ranks-monotonicity-spearman-rho-r_files/figure-html/viz_ex-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="bad-solution-linear-regression" class="section level3">
<h3>(Bad) Solution: Linear Regression</h3>
<p>Just at first thought, running a univariate <a href="https://en.wikipedia.org/wiki/Linear_regression">linear regression</a> might seem like a good way of attacking this problem. However, there are a couple of basic “gotchas” that make ordinary linear regression a not-so-great idea for this scenario:</p>
<ul>
<li><p>There are not many intervals (i.e. independent variables) per group. (This condition inherently makes any kind of model—not just a linear regression one—sensitive to the samples. In general, there is a problem with creating models with <a href="https://garstats.wordpress.com/2017/02/04/small-sample-sizes/">small sample sizes</a>.)</p></li>
<li><p>The values across groups might have very different magnitudes. (Thus, trying to create a single, general model that groups all of the data and uses the group labels as a categorical independent variable would likely lead to unreliable results.)</p></li>
<li><p>The values themselves might be very volatile for a single group. (This might be interpreted as a violation of the <a href="https://www.statisticssolutions.com/assumptions-of-linear-regression/">normal distribution assumption of linear regression</a>. Additionally, if the values are truly monotonic, the assumption of no autocorrelation might also be violated.)</p></li>
</ul>
<p>Aside from these caveats, the value for a given interval is not relevant—rather, its relationship with all other values is, and, more specifically, its relationships with the previous and subsequent values. <a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
</div>
<div id="better-solution-spearmans-rho" class="section level3">
<h3>(Better) Solution: <em>Spearman’s Rho</em></h3>
<p>Given the nature of the data (which one might say is non linear) and my intent to quantify ordinality between two variables, it turns out that <a href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient">Spearman’s rho</a>, in theory, provides exactly the measurement that I want—it quantifies the association between paired samples using the ranks of the variables (not their values) relative to their samples. Notably, the statistical significance (i.e. via a p-value) can be calculated (traditionally, using a <a href="https://en.wikipedia.org/wiki/Student%27s_t-test">t-test</a>), which should be handy for my intent on identifying non-triviality.</p>
<p>Nonetheless, even though this metric seems promisable, it will certainly be sensitive to the small samples of each group (assuming that it is calculated for each group). Don’t believe me? Check out how the Spearman’s rho value changes for the numerica columns in the built-in <code>iris</code> dataset (which has 150 rows) when it is calculated for just the first 10 rows.</p>
<p><img src="/post/2018-04-16-ranks-monotonicity-spearman-rho-r_files/figure-html/iris_corrr-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="another-solution-custom-heuristic" class="section level3">
<h3>Another Solution: Custom Heuristic</h3>
<p>So, what can be done? Well, even with the hope that the Spearman’s rho metric provides for quantification and significance inference, I thought that I would try to create some kind of easily understandable heuristic that I could explain to someone else without having to delve into statistical theory. Nonetheless, I would be ignorant to not compare (and validate) the results of my heurisitc with those of statistical theory after creating my heurisitc.</p>
<p>Having this plan in mind, I began to think about how I would define my heuristic, which, in essence, tries to quantify <a href="https://en.wikipedia.org/wiki/Monotonic_function"><strong>monotocity</strong></a>. But what exactly constitutes monoticity? Surprisingly, that’s a more complex question than it might seem. <a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> (For example, did you know that numbers may be strictly or weakly monotonic?)</p>
<p>For my purposes, I don’t necessarily care if the set of values is <em>strictly</em> increasing or decreasing, but they should be “sufficiently” increasing or decreasing. For example, while it is clear that the sequence <code>1</code>, <code>2</code>, <code>3</code>, <code>4</code> is strictly monotonic (increasing) and the sequence <code>1</code>, <code>2</code>, <code>4</code>, <code>3</code> is not, I would consider the latter “sufficiently” monotonic. On the the hand, I would consider something like <code>4</code>, <code>2</code>, <code>3</code>, <code>1</code> because the <code>1</code> and <code>4</code> are “badly” misplaced in one another’s appropriate places, which are at the extreme ends of the sequence. Moreover, if I was intent on identifying increasing monoticity (as opposed to decreasing monotonicity), I would consider <code>4</code>, <code>3</code>, <code>2</code>, <code>1</code> “bad”, even though it is strictly monotonically decreasing. But what about something like <code>1</code>, <code>4</code>, <code>3</code>, <code>2</code> (again, assuming that I am evaluating increasing monotonicity)? Even though the <code>2</code> and <code>4</code> are swapped, I might still consider this sequence “sufficiently” monotonic because the <code>1</code> and <code>3</code> are placed correctly and the <code>2</code> and <code>4</code> are “not too far apart”. Anyways, it’s easy to see how having some kind of formal definition/calculation/criteria for monotonicity is handy.</p>
<div id="the-algorithm" class="section level4">
<h4>The “Algorithm”</h4>
<p>After some thinking, I came up with the following algorithm (if one can even call it that).</p>
<p>(<strong>NOTE:</strong> I explicitly list the variable names that I use in the code that follows to help the reader understand the relationships between these steps and the implementation.)</p>
<ol style="list-style-type: decimal">
<li>Given an <code>n</code>-length sequence of arbitrary values, assign each value an integer value between <code>1</code> and <code>n</code> to represent its “actual” rank. This rank is to be assigned based on relative value in the set of <a href="https://en.wikipedia.org/wiki/Real_number">real numbers</a>. <a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></li>
</ol>
<ul>
<li>In the machine learning setting, this set of ranks is the dependent variable (i.e. <code>y</code>) to predict.</li>
<li>In the example situation described before, it equates to the rank that would be assigned to the quarterly interval based on sales relative to the other quarters.</li>
<li>In the simulation that I run below, this is the variable <code>y0</code>.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Create a matrix of all permutations of actual rank and “assigned” rank. To be clear, this “assigned” rank is independent of the actual rank and value. To make things straightforward, these assigned ranks should be transformed to use only the same set of possible rank values dictated by the actual ranks (i.e. integers between 1 and <code>n</code>).</li>
</ol>
<ul>
<li>In the machine learning setting, this “assigned” rank is the independent variable (i.e. <code>x</code>) used as a predictor.</li>
<li>In the example, it is the quarterly interval.</li>
<li>In the simulation that follows, assigned rank is <code>x0</code>, and the matrix (actually, a <code>tibble</code>) of combinations is <code>data_permn</code>.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><p>Calculate the absolute difference between the “actual” and “assigned” ranks for each value in the sequence. Subtract this distance from the maximum rank value in the sequence. The resulting value is what I call the “inverse monotonic distance” (<code>mntc_distinv</code> in the following code).</p></li>
<li><p>Repeat the calculation of inverse monotonic distance for all groups (<code>grp</code>) of “actual” (<code>y0</code>) and “assigned” (<code>x0</code>) ranks.</p></li>
<li><p>Sum up the inverse monotonic distance for each value in the permutation group and take the average of this sum for each group. <a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> Re-scale this per-group value to a 0 to 1 basis. <a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> (In the code that follows, I re-use the variable name <code>mntc_distinv</code> for this transformed value.)</p></li>
<li><p>Identify any group (<code>grp</code>) corresponding to a sum-averaged-re-scaled value (<code>mntc_distinv</code>) in the upper 50% quantile of all values (i.e. assigned the value <code>&quot;(0.5,1]&quot;</code> for the <code>mntc_tier2</code> variable) as <strong>“sufficiently” monotonic</strong>. (The <code>mntc_tier2</code> variable can be interpreted as my heuristic.)</p></li>
</ol>
<p>Notably, even though the result set is split at the 50% threshold (which is a subjective choice), this does not mean that 50% of all possible groups are classified as 50%. (According to my method, only 33% are for <code>n = 4</code>.)</p>
</div>
</div>
</div>
<div id="implementing-the-heuristic" class="section level2">
<h2>Implementing the Heuristic</h2>
<p>Ok, that is enough discussion. What follows is the implementation.</p>
<p><em>NOTE:</em> In order to keep focus on how the code implements methodology, I recommend reviewing the code but not worrying too much about the details (such as the internal workings of my custom functions). Rather, I’d recommend inspecting in detail only the parts that are printed out (and going back later to understand the complexities, if curious).</p>
<pre class="r"><code>library(&quot;dplyr&quot;)
library(&quot;ggplot2&quot;)
library(&quot;tidyr&quot;)
# These packages are used, but their functions are called explicitly.
# library(&quot;purrr&quot;)
# library(&quot;broom&quot;)
# library(&quot;combinat&quot;)</code></pre>
<p>The only choice that I need to make to begin is the length of the set of values (i.e. <code>n</code>), which should be a “small” integer. I’ll choose <code>4</code>, simply because <code>3</code> seems like it is “too small” and because subsequent visualization(s) becomes “cluttered” (and interpretation becomes less direct) if a number <code>5</code> or greater is chosen. <a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> (Nonetheless, the methodology and results remain valid for any integer.)</p>
<p>The following code chunk corresponds to steps 1 and 2 in my methodology, which are basically just set-up steps.</p>
<pre class="r"><code>create_permns &lt;- function(n = 1L) {
  
  n_seq &lt;- seq(1L, n, by = 1L)
  
  combs &lt;-
    combinat::permn(n_seq) %&gt;%
    purrr::map( ~ paste(.x, collapse = &quot;&quot;)) %&gt;%
    unlist() %&gt;%
    as.integer()
  data_xy &lt;-
    tibble(grp_x = combs, grp_y = combs) %&gt;%
    expand(grp_x, grp_y) %&gt;%
    mutate(grp = paste0(&quot;x&quot;, grp_x, &quot;y&quot;, grp_y))
  
  into_seq &lt;- seq(1L, n, by = 1L) %&gt;% as.character()
  sep_seq &lt;- seq(1L, n - 1L, by = 1L)
  wrangle_data_xy &lt;-
    function(data = NULL, which = NULL) {
      col_grp &lt;- rlang::sym(paste0(&quot;grp_&quot;, which))
      col_0 &lt;- rlang::sym(paste0(which, &quot;0&quot;))
      data %&gt;%
        separate(!!col_grp, into_seq, sep = sep_seq, remove = FALSE) %&gt;%
        gather(idx, !!col_0, matches(&quot;^[0-9]$&quot;)) %&gt;%
        mutate_at(vars(idx, !!col_0), funs(as.integer))
    }
  inner_join(data_xy %&gt;% wrangle_data_xy(&quot;x&quot;),
               data_xy %&gt;% wrangle_data_xy(&quot;y&quot;)) %&gt;%
    select(-idx) %&gt;%
    arrange(grp_x, grp_y)
}

data_permns &lt;- create_permns(n = n)
data_permns</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
grp_x
</th>
<th style="text-align:right;">
grp_y
</th>
<th style="text-align:left;">
grp
</th>
<th style="text-align:right;">
x0
</th>
<th style="text-align:right;">
y0
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1234
</td>
<td style="text-align:left;">
x1234y1234
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1234
</td>
<td style="text-align:left;">
x1234y1234
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1234
</td>
<td style="text-align:left;">
x1234y1234
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1234
</td>
<td style="text-align:left;">
x1234y1234
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1243
</td>
<td style="text-align:left;">
x1234y1243
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1243
</td>
<td style="text-align:left;">
x1234y1243
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1243
</td>
<td style="text-align:left;">
x1234y1243
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
4
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1243
</td>
<td style="text-align:left;">
x1234y1243
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1324
</td>
<td style="text-align:left;">
x1234y1324
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1324
</td>
<td style="text-align:left;">
x1234y1324
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
3
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; border:0;" colspan="100%">
<sup>1</sup> # of total rows: 2304
</td>
</tr>
</tfoot>
</table>
<p>Note(s) about the above code chunk:</p>
<ul>
<li><p>Although the “actual” ranks (<code>y0</code>) could be any real numbers, I’m using values between <code>1</code> and <code>n</code> because my methodology dictates that the arbitrary set of values would need to be transformed to this range anyways. When transformed to this range, the values should be interpreted as ranks.</p></li>
<li><p>Like the “actual” <code>y0</code> ranks (representing the order of the original, arbitrary values), the <code>x0</code> ranks could technically be any real numbers, but they would need to be transformed to the <code>1</code>-to-<code>n</code> range anyways, so I do that directly.</p></li>
<li><p>The number of combinations of “actual” (<code>y0</code>) and “assigned” (<code>x0</code>) rank pairs is equal to <code>n! * n!</code> (i.e. 576). For my implementation, the data.frame <code>data_permns</code> actually has <code>n! * n! * n</code> (2304) rows (because it is arranged in a “long” format).</p></li>
<li><p><code>grp_x</code> and <code>grp_y</code> (and the combination of the two in the <code>grp</code> column) identify the <code>n</code>-length groups of pairs of <code>x0</code> and <code>y0</code> ranks. These are primarily useful for human interpretability and are not actually relevant for computations.</p></li>
<li><p>I use a function here (and elsewhere) although it may seem unnecessary for a single execution because I’ll repeat the methodology for different values of <code>n</code> later.</p></li>
</ul>
<p>Now, I implement the initial calculation of “inverse monotonic distance” (<code>mntc_distinv</code>).</p>
<pre class="r"><code>add_mntc_cols &lt;- function(data = NULL) {
  data %&gt;%
    group_by(grp) %&gt;%
    arrange(x0, .by_group = TRUE) %&gt;% 
    mutate(mntc = ifelse((y0 == cummax(y0)) | (y0 == cummin(y0)), 1L, 0L)) %&gt;% 
    mutate(mntc_distinv = as.integer(x0 * (max(x0) - abs(x0 - y0)))) %&gt;% 
    ungroup()
}
data_mntc &lt;- add_mntc_cols(data_permns)
data_mntc</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
grp_x
</th>
<th style="text-align:right;">
grp_y
</th>
<th style="text-align:left;">
grp
</th>
<th style="text-align:right;">
x0
</th>
<th style="text-align:right;">
y0
</th>
<th style="text-align:right;">
mntc
</th>
<th style="text-align:right;">
mntc_distinv
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1234
</td>
<td style="text-align:left;">
x1234y1234
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1234
</td>
<td style="text-align:left;">
x1234y1234
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
8
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1234
</td>
<td style="text-align:left;">
x1234y1234
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
12
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1234
</td>
<td style="text-align:left;">
x1234y1234
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
16
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1243
</td>
<td style="text-align:left;">
x1234y1243
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1243
</td>
<td style="text-align:left;">
x1234y1243
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
8
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1243
</td>
<td style="text-align:left;">
x1234y1243
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
9
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1243
</td>
<td style="text-align:left;">
x1234y1243
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
12
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1324
</td>
<td style="text-align:left;">
x1234y1324
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
</tr>
<tr>
<td style="text-align:right;">
1234
</td>
<td style="text-align:right;">
1324
</td>
<td style="text-align:left;">
x1234y1324
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
6
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; border:0;" colspan="100%">
<sup>1</sup> # of total rows: 2304
</td>
</tr>
</tfoot>
</table>
<p>Note(s) about the above code chunk:</p>
<ul>
<li>The <code>mntc</code> variable is a “running” binary <code>1</code> or <code>0</code> to indicate whether or not <code>y0</code> is monotonic up through its position in the sequence. (It does not differentiate between increasing or decreasing.)</li>
<li>I use the <code>integer</code> data type (as opposed to simply <code>numeric</code>) where possible because it is more memory efficient (although memory efficiency is not a concern with this data).</li>
</ul>
<p>Next is the calculation of the transformed (i.e. summed-averaged-re-scaled) version of the “inverse monotonic distance” (<code>mntc_distinv</code>), as well as the split of the <code>mntc_distinv</code> into upper and lower 50% quantiles (<code>mntc_tier2</code>).</p>
<pre class="r"><code>unitize &lt;- function(x = NULL) {
  (x - min(x)) / (max(x) - min(x))
}
summarise_mntc &lt;- function(data = NULL) {
  data %&gt;%
    group_by(grp) %&gt;% 
    summarise_at(vars(mntc_distinv), funs(mean)) %&gt;% 
    ungroup() %&gt;% 
    mutate_at(vars(mntc_distinv), funs(unitize)) %&gt;% 
    mutate(mntc_tier2 = cut(mntc_distinv, 2))
}
summ_mntc &lt;- summarise_mntc(data_mntc)
summ_mntc</code></pre>
<p>Now, with the “algorithm” fully implemented, I can begin to evaluate the results.</p>
<p>Exactly how many values make up each 50% quantile?</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
mntc_tier2
</th>
<th style="text-align:right;">
n
</th>
<th style="text-align:right;">
n_pct
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
384
</td>
<td style="text-align:right;">
66.67
</td>
</tr>
<tr>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
192
</td>
<td style="text-align:right;">
33.33
</td>
</tr>
</tbody>
</table>
<p>What does the distribution of all “inverse monotonic distance” values look like?</p>
<p><img src="/post/2018-04-16-ranks-monotonicity-spearman-rho-r_files/figure-html/viz_mntc_distinv-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><img src="/post/2018-04-16-ranks-monotonicity-spearman-rho-r_files/figure-html/viz_summ_mntc-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>The positive identificaiton (in yellow) of combinations along the left-to-right, lower-to-upper diagonal is what I would expect. These are the values where <code>x0</code> and <code>y0</code> are perfectly matched. Conversely, values along the other diagonal are generally colored in purple, as I would expect. These combinations consist of sequences of <code>x0</code> and <code>y0</code> values that are “negatively” symmetric (e.g. (<code>1</code>, <code>2</code>, <code>3,</code> <code>4</code>) and (<code>4</code>, <code>3</code>, <code>2</code>, <code>1</code>)).</p>
</div>
<div id="checking-the-heuristic" class="section level2">
<h2>Checking the Heuristic</h2>
<p>Ok, my heuristic seems valid, but how can I know for sure that it is reasonable? I mentioned before that Spearman’s rho should serve a good measure, so I’ll take a look at it now.</p>
<pre class="r"><code>add_cortest_cols &lt;- function(data = NULL) {
  data %&gt;%
    group_by(grp) %&gt;%
    nest() %&gt;%
    mutate(cortest =
             purrr::map(data, ~ broom::tidy(cor.test(.$x0, .$y0, method = &quot;spearman&quot;)))
    ) %&gt;%
    unnest(cortest, .drop = TRUE) %&gt;%
    select(grp, estimate, p.value)
}
summarise_mntc_wcortest &lt;- function(data = NULL) {
   summ &lt;- summarise_mntc(data)
   data %&gt;%
     add_cortest_cols() %&gt;% 
     inner_join(summ, by = &quot;grp&quot;)
}

summ_mntc_wcortest &lt;- summarise_mntc_wcortest(data_mntc)
summ_mntc_wcortest</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
grp
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
p.value
</th>
<th style="text-align:right;">
mntc_distinv
</th>
<th style="text-align:left;">
mntc_tier2
</th>
<th style="text-align:right;">
n_pct
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
x1234y1234
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:right;">
0.08
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left;">
x1234y1243
</td>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:right;">
0.33
</td>
<td style="text-align:right;">
0.67
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left;">
x1234y1324
</td>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:right;">
0.33
</td>
<td style="text-align:right;">
0.76
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left;">
x1234y1342
</td>
<td style="text-align:right;">
0.4
</td>
<td style="text-align:right;">
0.75
</td>
<td style="text-align:right;">
0.38
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left;">
x1234y1423
</td>
<td style="text-align:right;">
0.4
</td>
<td style="text-align:right;">
0.75
</td>
<td style="text-align:right;">
0.48
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left;">
x1234y1432
</td>
<td style="text-align:right;">
0.2
</td>
<td style="text-align:right;">
0.92
</td>
<td style="text-align:right;">
0.43
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left;">
x1234y2134
</td>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:right;">
0.33
</td>
<td style="text-align:right;">
0.86
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left;">
x1234y2143
</td>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
0.42
</td>
<td style="text-align:right;">
0.52
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left;">
x1234y2314
</td>
<td style="text-align:right;">
0.4
</td>
<td style="text-align:right;">
0.75
</td>
<td style="text-align:right;">
0.57
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left;">
x1234y2341
</td>
<td style="text-align:right;">
-0.2
</td>
<td style="text-align:right;">
0.92
</td>
<td style="text-align:right;">
0.14
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
100
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; border:0;" colspan="100%">
<sup>1</sup> # of total rows: 576
</td>
</tr>
</tfoot>
</table>
<p>What exactly is the distribution of the Pearson’s rho t-test estimates and p-values?</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
abs(estimate)
</th>
<th style="text-align:right;">
p.value
</th>
<th style="text-align:right;">
n
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.4
</td>
<td style="text-align:right;">
0.75
</td>
<td style="text-align:right;">
192
</td>
</tr>
<tr>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:right;">
0.33
</td>
<td style="text-align:right;">
144
</td>
</tr>
<tr>
<td style="text-align:right;">
0.2
</td>
<td style="text-align:right;">
0.92
</td>
<td style="text-align:right;">
96
</td>
</tr>
<tr>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
48
</td>
</tr>
<tr>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
0.42
</td>
<td style="text-align:right;">
48
</td>
</tr>
<tr>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:right;">
0.08
</td>
<td style="text-align:right;">
48
</td>
</tr>
</tbody>
</table>
<p>Note(s) about the above output:</p>
<ul>
<li>By taking the absolute value of the estimate, I am essentially treating monotonically increasing and decreasing as equal.</li>
<li>There are a relatively “small” number of distinct values. <a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></li>
</ul>
<p>Now, to understand how the Pearson’s rho t-test <code>estimate</code>s and <code>p.value</code>s correspond to my heuristic, I’ll simply overlay the combinations that are identified as significant to my previous heat map of rank combinations. Because I’m erring on the side of flexibility in defining “sufficient” monotonicity, I’ll say that the pairs corresponding to the bottom two tiers of p-values (corresponding to <code>0.0833</code> and <code>0.33</code>) constitute “sufficient” monoticity.</p>
<p><img src="/post/2018-04-16-ranks-monotonicity-spearman-rho-r_files/figure-html/viz_summ_mntc_wcortest-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>It looks like there is a large amount of overlap between my heuristic classification of “sufficient” monoticity and that identified by a more statistical approach.</p>
<p>Now I’ll repeat the simulation for other values of <code>n</code>. (Because computations start to become intensive with <code>n = 6</code>, and because the <code>n = 2</code> is relatvily trivial, I’ll evaluate values of <code>3</code>, <code>4</code>, and <code>5</code> for <code>n</code>. <a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a>)</p>
<pre class="r"><code>ns &lt;- tibble(n = 3L:5L)
summ_mntc_byn &lt;-
  ns %&gt;% 
  mutate(data = purrr::map(n, ~(create_permns(.x) %&gt;% add_mntc_cols()))) %&gt;% 
  mutate(summ = purrr::map(data, summarise_mntc_wcortest)) %&gt;% 
  unnest(summ, .drop = TRUE) %&gt;% 
  ungroup() %&gt;% 
  arrange(n)</code></pre>
<p>What is the breakdown of <code>mntc_tier2</code> values?</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
n
</th>
<th style="text-align:left;">
mntc_tier2
</th>
<th style="text-align:right;">
nn
</th>
<th style="text-align:right;">
nn_pct
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
66.7
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
33.3
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
384
</td>
<td style="text-align:right;">
66.7
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
192
</td>
<td style="text-align:right;">
33.3
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
9720
</td>
<td style="text-align:right;">
67.5
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
4680
</td>
<td style="text-align:right;">
32.5
</td>
</tr>
</tbody>
</table>
<p>What about the distribution of <code>mntc_distinv</code> values? And of the <code>estimate</code>s and <code>p.value</code>s?</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
n
</th>
<th style="text-align:right;">
abs(estimate)
</th>
<th style="text-align:right;">
p.value
</th>
<th style="text-align:left;">
mntc_tier2
</th>
<th style="text-align:right;">
nn
</th>
<th style="text-align:right;">
nn_pct
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
50.0
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:right;">
0.3
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
16.7
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
16.7
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:right;">
0.3
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
16.7
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
48
</td>
<td style="text-align:right;">
8.3
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.2
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
72
</td>
<td style="text-align:right;">
12.5
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.4
</td>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
144
</td>
<td style="text-align:right;">
25.0
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
0.4
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
4.2
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:right;">
0.3
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
72
</td>
<td style="text-align:right;">
12.5
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
4.2
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.2
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
4.2
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.4
</td>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
48
</td>
<td style="text-align:right;">
8.3
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
0.4
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
4.2
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:right;">
0.3
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
72
</td>
<td style="text-align:right;">
12.5
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:left;">
(0.5,1]
</td>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
4.2
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
600
</td>
<td style="text-align:right;">
4.2
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
1800
</td>
<td style="text-align:right;">
12.5
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0.2
</td>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
1440
</td>
<td style="text-align:right;">
10.0
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0.3
</td>
<td style="text-align:right;">
0.7
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
1680
</td>
<td style="text-align:right;">
11.7
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0.4
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:left;">
(-0.001,0.5]
</td>
<td style="text-align:right;">
600
</td>
<td style="text-align:right;">
4.2
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; border:0;" colspan="100%">
<sup>1</sup> # of total rows: 36
</td>
</tr>
</tfoot>
</table>
<p><img src="/post/2018-04-16-ranks-monotonicity-spearman-rho-r_files/figure-html/viz_mntc_distinv_byn-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><img src="/post/2018-04-16-ranks-monotonicity-spearman-rho-r_files/figure-html/viz_mntc_tier2_byn-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The distributions are sparse due to the relatively small number of unique values for each metric (<code>mntc_distinv</code>, <code>p.value</code>, etc.). <a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> Consequently, it is a bit difficult to extract much meaningful insight about the relationships among the metrics. To really understand how the distributions and relationships scale with larger values of <code>n</code>, mathematical theory would need to be applied.</p>
<p>Nonethless, without jumping more into statistical theory, It seems to me that the identification of rank combinations as significant by my heuristic classification and Spearman’s rho (assuming that one uses the traditional p-value-below-a-thresshold approach) would become <strong>more dissimilar</strong> as the value of <code>n</code> increases. This is because my classification simply splits all possible values into two sets for any value of <code>n</code>, meaning that the percentage of all possible combinations is relatively insensitive to the value of <code>n</code>. <a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a> On the other hand, the <em>Spearman’s rho</em> p-values would become more refined with larger values of <code>n</code>.</p>
<p>Anyways, I believe that my heuristic serves my purposes well. I only really intended it to be used for small values of <code>n</code>. Also, I intended to create a “relaxed” definition of monotonocity, so having only a very small percentage of all possible rank combinations meet the criteria would have actually been undesireable.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>In the end, I think I did more work than I really needed to do to answer my original question about quantifying monotonocity and inferring significance, but I think, in all, this was a worthwhile exploration.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The market analyst does not necessarily hypothesize why annual projections tend to be high (i.e. perhaps due to over-confidence)<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Additionally, prediction is not the concern—rather, quantification of trend is. (While regression certainly can help with trend identification, its capability to create predictions is perhaps its better use.)<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>I’ll leave the reader to dive into all of the theory.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>In reality, the rank values could also be any arbitrary value on the real number scale.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>A sum of sums (instead of an average of sums) could be used here and the subsequent results would not change.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>This is not completely necessary, but I believe that it makes the computation(s) and the calculated values “generalizable” for any <code>n</code>-length sequence.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>I realized this through some trial and error.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Here, there are only <code>n + 1</code> (i.e. 5 unique <code>abs(estimate)</code>s and <code>p.value</code>s. This result is <strong>not</strong> generally true. (For example, when choosing <code>n = 5</code>, there will be more than <code>6</code> unique values of each metric.)<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>This presents a good opportunity to implement a version of the “nest-mutate-unnest” idiom that can be very effective for creating many models. The <a href="http://r4ds.had.co.nz/many-models.html">“many models” chapter</a> in the <a href="http://r4ds.had.co.nz"><em>R For Data Science</em> book</a> provides an excellent example of this process.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>Unfortunately this is due to the nature of the data and the simulation, so nothing can be done about it.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>Note that the 33% number found for <code>n = 4</code> is not generally true, although this percentage does not seem to change drastically with different values of <code>n</code>.<a href="#fnref11">↩</a></p></li>
</ol>
</div>
